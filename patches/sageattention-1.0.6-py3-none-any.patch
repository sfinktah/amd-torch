diff -ru --ignore-trailing-space /tmp/tmp.kjS06YTEbm/unzipped/sageattention/attn_qk_int8_per_block.py sageattention/attn_qk_int8_per_block.py
--- /tmp/tmp.kjS06YTEbm/unzipped/sageattention/attn_qk_int8_per_block.py	2024-11-12 02:54:20.000000000 +1100
+++ sageattention/attn_qk_int8_per_block.py	2025-07-26 03:58:30.875251200 +1000
@@ -2,60 +2,116 @@
 import triton
 import triton.language as tl
 
+# =================== Autotuning Configuration ===================
+# These kernel configurations are designed to support a wide range
+# of AMD GPUs. They vary BLOCK sizes, number of stages, warps, and waves_per_eu.
+# This tuning space is especially important for AMD architectures like:
+#
+# - RDNA2: NAVI21–NAVI24, VANGOGH, REMBRANDT, RAPHAEL (gfx103x series)
+# - RDNA3: NAVI31–NAVI33, PHOENIX1/2, STRIX1, STRIX_HALO, KRACKAN1 (gfx11xx)
+# - Future RDNA4: NAVI44, NAVI48 (gfx12xx)
+#
+# Many of these platforms (especially RDNA2/3) have strict limits on shared memory (64 KiB)
+# and benefit from conservative kernel configs that avoid overprovisioning warps or block sizes.
+
+configs = [
+    triton.Config(
+        {
+            'BLOCK_M': BM,
+            'BLOCK_N': BN,
+            'STAGE': S,
+            'waves_per_eu': wpe,
+            'HEAD_DIM': hd,  # <-- ADD THIS
+        },
+        num_warps=nw,
+        num_stages=ns
+    )
+    for BM in [32, 64]
+    for BN in [16, 32, 64]
+    for nw in [2, 4, 8]
+    for ns in [1, 2, 3, 4]
+    for S in [1, 2, 3]
+    for wpe in [2, 3, 4]
+    for hd in [64, 128, 256]
+]
+
+# =================== Configuration Filter ===================
+def keep(conf):
+    BLOCK_M = conf.kwargs["BLOCK_M"]
+    BLOCK_N = conf.kwargs["BLOCK_N"]
+    HEAD_DIM = conf.kwargs.get("HEAD_DIM")
+    shm_bytes = BLOCK_M * HEAD_DIM * conf.num_stages * 8 // 3
+    if shm_bytes > 65536:
+        print(f"[SKIP] BLOCK_M={BLOCK_M}, HEAD_DIM={HEAD_DIM}, num_stages={conf.num_stages} "
+              f"uses {shm_bytes}B > 64KiB")
+        return False
+
+    BLOCK_AREA = BLOCK_M * BLOCK_N
+    if BLOCK_AREA > 4096 or BLOCK_M < BLOCK_N or BLOCK_M // BLOCK_N >= 8:
+        return False
+    if BLOCK_AREA >= 2048:
+        if conf.num_warps < 4 or conf.num_stages > 2:
+            return False
+    elif BLOCK_AREA >= 1024:
+        if conf.num_warps < 2 or conf.num_stages > 3:
+            return False
+    else:
+        if conf.num_stages > 4:
+            return False
+    if conf.kwargs.get('waves_per_eu', 4) > 4:
+        return False
+    print(f"[AUTOTUNE] BLOCK_M={BLOCK_M}, BLOCK_N={BLOCK_N}, "
+          f"num_warps={conf.num_warps}, num_stages={conf.num_stages}, "
+          f"STAGE={conf.kwargs['STAGE']}, waves_per_eu={conf.kwargs['waves_per_eu']}")
+    return True
+
 @triton.jit
 def _attn_fwd_inner(acc, l_i, m_i, q, q_scale, kv_len,
                     K_ptrs, K_scale_ptr, V_ptrs, stride_kn, stride_vn, 
                     start_m,  
                     BLOCK_M: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_N: tl.constexpr,  
-                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr,  
-                    ):
+                    STAGE: tl.constexpr, offs_m: tl.constexpr, offs_n: tl.constexpr):
     lo, hi = 0, kv_len
     for start_n in range(lo, hi, BLOCK_N):
         start_n = tl.multiple_of(start_n, BLOCK_N)
         k_mask = offs_n[None, :] < (kv_len - start_n)   
-        k = tl.load(K_ptrs, mask = k_mask)
+        k = tl.load(K_ptrs, mask=k_mask)
         k_scale = tl.load(K_scale_ptr)
         qk = tl.dot(q, k).to(tl.float32) * q_scale * k_scale 
         m_ij = tl.maximum(m_i, tl.max(qk, 1))
         qk = qk - m_ij[:, None]
         p = tl.math.exp2(qk)
         l_ij = tl.sum(p, 1)
-        
         alpha = tl.math.exp2(m_i - m_ij)
         l_i = l_i * alpha + l_ij
-        
         acc = acc * alpha[:, None]
-        
-        v = tl.load(V_ptrs, mask = offs_n[:, None] < (kv_len - start_n))
+        v = tl.load(V_ptrs, mask=offs_n[:, None] < (kv_len - start_n))
         p = p.to(tl.float16)
-        
-        acc += tl.dot(p, v, out_dtype=tl.float16)   
+        acc += tl.dot(p, v, out_dtype=tl.float32)
         m_i = m_ij
         K_ptrs += BLOCK_N * stride_kn
         K_scale_ptr += 1
         V_ptrs += BLOCK_N * stride_vn
     return acc, l_i
 
+@triton.autotune(list(filter(keep, configs)), key=['qo_len', 'kv_len', 'h_qo'])
 @triton.jit
 def _attn_fwd(Q, K, V, Q_scale, K_scale, Out,  
               stride_qz, stride_qh, stride_qn,
               stride_kz, stride_kh, stride_kn,  
               stride_vz, stride_vh, stride_vn,  
               stride_oz, stride_oh, stride_on,  
-              qo_len, kv_len, H: tl.constexpr, num_kv_groups: tl.constexpr,
+              qo_len, kv_len,
+              H: tl.constexpr, num_kv_groups: tl.constexpr,
               HEAD_DIM: tl.constexpr,  
               BLOCK_M: tl.constexpr,  
               BLOCK_N: tl.constexpr,  
-              STAGE: tl.constexpr
-              ):
+              STAGE: tl.constexpr):
     start_m = tl.program_id(0)
-
     off_z = tl.program_id(2).to(tl.int64)
     off_h = tl.program_id(1).to(tl.int64)
-
     q_scale_offset = (off_z * H + off_h) * tl.cdiv(qo_len, BLOCK_M)
     k_scale_offset = (off_z * (H // num_kv_groups) + off_h // num_kv_groups) * tl.cdiv(kv_len, BLOCK_N)  
-    
     offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)
     offs_n = tl.arange(0, BLOCK_N)
     offs_k = tl.arange(0, HEAD_DIM)
@@ -65,32 +121,22 @@
     K_scale_ptr = K_scale + k_scale_offset
     V_ptrs = V + (off_z * stride_vz + (off_h // num_kv_groups) * stride_vh) + offs_n[:, None] * stride_vn + offs_k[None, :]
     O_block_ptr = Out + (off_z * stride_oz + off_h * stride_oh) + offs_m[:, None] * stride_on + offs_k[None, :]
-    
-    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float("inf")
-    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0
+    m_i = tl.full([BLOCK_M], -float("inf"), dtype=tl.float32)
+    l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)
     acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)
-    
-    q = tl.load(Q_ptrs, mask = offs_m[:, None] < qo_len)
+    q = tl.load(Q_ptrs, mask=offs_m[:, None] < qo_len)
     q_scale = tl.load(Q_scale_ptr)
-    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, kv_len, K_ptrs, K_scale_ptr, V_ptrs, stride_kn, stride_vn,
-                                    start_m,  
-                                    BLOCK_M, HEAD_DIM, BLOCK_N,  
-                                    4 - STAGE, offs_m, offs_n 
-                                    )
+    acc, l_i = _attn_fwd_inner(acc, l_i, m_i, q, q_scale, kv_len,
+                               K_ptrs, K_scale_ptr, V_ptrs, stride_kn, stride_vn,
+                               start_m, BLOCK_M, HEAD_DIM, BLOCK_N, STAGE, offs_m, offs_n)
     acc = acc / l_i[:, None]
-    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < qo_len))
+    tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask=offs_m[:, None] < qo_len)
 
 def forward(q, k, v, q_scale, k_scale, tensor_layout="HND", output_dtype=torch.float16):
-    BLOCK_M = 128
-    BLOCK_N = 64
-    stage = 1
-
     o = torch.empty(q.shape, dtype=output_dtype, device=q.device)
-
     if tensor_layout == "HND":
         b, h_qo, qo_len, head_dim = q.shape
         _, h_kv, kv_len, _ = k.shape
-
         stride_bz_q, stride_h_q, stride_seq_q = q.stride(0), q.stride(1), q.stride(2)
         stride_bz_k, stride_h_k, stride_seq_k = k.stride(0), k.stride(1), k.stride(2)
         stride_bz_v, stride_h_v, stride_seq_v = v.stride(0), v.stride(1), v.stride(2)
@@ -98,18 +144,15 @@
     elif tensor_layout == "NHD":
         b, qo_len, h_qo, head_dim = q.shape
         _, kv_len, h_kv, _ = k.shape
-
         stride_bz_q, stride_h_q, stride_seq_q = q.stride(0), q.stride(2), q.stride(1)
         stride_bz_k, stride_h_k, stride_seq_k = k.stride(0), k.stride(2), k.stride(1)
         stride_bz_v, stride_h_v, stride_seq_v = v.stride(0), v.stride(2), v.stride(1)
         stride_bz_o, stride_h_o, stride_seq_o = o.stride(0), o.stride(2), o.stride(1)
     else:
         raise ValueError(f"tensor_layout {tensor_layout} not supported")
-    
     HEAD_DIM_K = head_dim
     num_kv_groups = h_qo // h_kv
-
-    grid = (triton.cdiv(qo_len, BLOCK_M), h_qo, b)
+    grid = lambda META: (triton.cdiv(qo_len, META['BLOCK_M']), h_qo, b)
     _attn_fwd[grid](
         q, k, v, q_scale, k_scale, o,  
         stride_bz_q, stride_h_q, stride_seq_q, 
@@ -118,8 +161,6 @@
         stride_bz_o, stride_h_o, stride_seq_o,
         qo_len, kv_len,
         h_qo, num_kv_groups,
-        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, HEAD_DIM=HEAD_DIM_K,  
-        STAGE=stage,  
-        num_warps=4 if head_dim == 64 else 8,
-        num_stages=3 if head_dim == 64 else 4)
+        HEAD_DIM=HEAD_DIM_K,
+    )
     return o
\ No newline at end of file
diff -ru --ignore-trailing-space /tmp/tmp.kjS06YTEbm/unzipped/sageattention/attn_qk_int8_per_block_causal.py sageattention/attn_qk_int8_per_block_causal.py
--- /tmp/tmp.kjS06YTEbm/unzipped/sageattention/attn_qk_int8_per_block_causal.py	2024-11-12 02:54:20.000000000 +1100
+++ sageattention/attn_qk_int8_per_block_causal.py	2025-07-26 03:58:30.875251200 +1000
@@ -44,7 +44,7 @@
         v = tl.load(V_ptrs, mask = offs_n[:, None] < (kv_len - start_n))
         p = p.to(tl.float16)
         
-        acc += tl.dot(p, v, out_dtype=tl.float16)   
+        acc += tl.dot(p, v, out_dtype=tl.float32)   # zlp
         m_i = m_ij
         K_ptrs += BLOCK_N * stride_kn
         K_scale_ptr += 1
@@ -102,8 +102,8 @@
     tl.store(O_block_ptr, acc.to(Out.type.element_ty), mask = (offs_m[:, None] < qo_len))
 
 def forward(q, k, v, q_scale, k_scale, tensor_layout="HND", output_dtype=torch.float16):
-    BLOCK_M = 128
-    BLOCK_N = 64
+    BLOCK_M = 64 #zlp
+    BLOCK_N = 64 #zlp
     stage = 3
 
     o = torch.empty(q.shape, dtype=output_dtype, device=q.device)
diff -ru --ignore-trailing-space /tmp/tmp.kjS06YTEbm/unzipped/sageattention/quant_per_block.py sageattention/quant_per_block.py
--- /tmp/tmp.kjS06YTEbm/unzipped/sageattention/quant_per_block.py	2024-11-15 03:34:50.000000000 +1100
+++ sageattention/quant_per_block.py	2025-07-26 03:58:30.875251200 +1000
@@ -30,7 +30,7 @@
     tl.store(output_ptrs, x_int8, mask=offs_n[:, None] < L)
     tl.store(scale_ptrs, scale)
 
-def per_block_int8(q, k, BLKQ=128, BLKK=64, sm_scale=None, tensor_layout="HND"):
+def per_block_int8(q, k, BLKQ=64, BLKK=64, sm_scale=None, tensor_layout="HND"):
     q_int8 = torch.empty(q.shape, dtype=torch.int8, device=q.device)
     k_int8 = torch.empty(k.shape, dtype=torch.int8, device=k.device)
 
