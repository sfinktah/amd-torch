diff -ru --ignore-trailing-space /tmp/tmp.OOB7NJEiKv/unzipped/torch/_higher_order_ops/triton_kernel_wrap.py torch/_higher_order_ops/triton_kernel_wrap.py
--- /tmp/tmp.OOB7NJEiKv/unzipped/torch/_higher_order_ops/triton_kernel_wrap.py	2025-04-15 23:15:28.000000000 +1000
+++ torch/_higher_order_ops/triton_kernel_wrap.py	2025-07-23 00:51:26.698337800 +1000
@@ -211,8 +211,15 @@
     # ignore backend-specific kwargs same way as in the native Triton code
     # https://github.com/triton-lang/triton/blob/a6bb57d6285e723c58e87dd7cba263db6efff789/python/triton/runtime/jit.py#L594-L596
     # why this is important for user-defined Triton kernels on AMD: https://github.com/pytorch/pytorch/issues/140800
+    # sfinktah notes:
+    #   see also triton/compiler/compiler.py
+    #   see also torch/_inductor/runtime/triton_heuristics.py
+    #   see also torch/_higher_order_ops/triton_kernel_wrap.py
     for name in list(kwargs):
         if name not in kernel.arg_names and name in options.__dict__:
+            arg_val = kwargs[name]
+            print("\033[93m%%% [debug] generate_ttir: removed invalid kernel arg: {}={} \033[0m -- torch/_higher_order_ops/triton_kernel_wrap.py".format(str(name), str(arg_val)))
+            #  print("\033[03m%%% [debug] valid kernel arg: {} \033[0m".format(str(kernel.arg_names)))
             kwargs.pop(name)
 
     if len(kwargs) != len(kernel.arg_names):
@@ -876,6 +883,8 @@
     kwargs = kwargs.copy()
     constant_args = constant_args.copy()
     for name in kernel.arg_names:
+        if name == 'waves_per_eu':
+            print("\033[93m%%% [debug] triton_kernel_wrapper_mutation_dense: found waves_per_au\033[0m -- torch/_higher_order_ops/triton_kernel_wrap.py")
         if name in kwargs:
             args.append(kwargs.pop(name))
         elif name in constant_args:
diff -ru --ignore-trailing-space /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/runtime/triton_compat.py torch/_inductor/runtime/triton_compat.py
--- /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/runtime/triton_compat.py	2025-04-15 23:15:30.000000000 +1000
+++ torch/_inductor/runtime/triton_compat.py	2025-07-24 22:20:37.771064200 +1000
@@ -1,3 +1,4 @@
+# incorporating fixes from https://github.com/pytorch/pytorch/blob/c90e23eb73212186b7eb9a98ba69e6309d864c67/torch/_inductor/runtime/triton_compat.py
 from __future__ import annotations
 
 from typing import Any, Union
@@ -44,7 +45,7 @@
             return (backend, arch)
 
     # In the latest triton, math functions were shuffled around into different modules:
-    # https://github.com/openai/triton/pull/3172
+    # https://github.com/triton-lang/triton/pull/3172
     try:
         from triton.language.extra import libdevice
 
@@ -68,6 +69,13 @@
         def _log2(x: Any) -> Any:
             raise NotImplementedError
 
+    HAS_WARP_SPEC = hasattr(tl, "async_task")
+
+    try:
+        from triton import knobs
+        print("\033[93m%%% [backported] imported triton knobs\033[0m")
+    except ImportError:
+        knobs = None
 else:
 
     def _raise_error(*args: Any, **kwargs: Any) -> Any:
@@ -87,6 +95,7 @@
     _log2 = _raise_error
     libdevice = None
     math = None
+    knobs = None
 
     class triton:  # type: ignore[no-redef]
         @staticmethod
@@ -101,6 +110,8 @@
         tensor = Any
         dtype = Any
 
+    HAS_WARP_SPEC = False
+
 
 def cc_warp_size(cc: Union[str, int]) -> int:
     if torch.version.hip:
@@ -135,4 +146,5 @@
     "math",
     "triton",
     "cc_warp_size",
+    "knobs",
 ]
diff -ru --ignore-trailing-space /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/runtime/triton_heuristics.py torch/_inductor/runtime/triton_heuristics.py
--- /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/runtime/triton_heuristics.py	2025-04-15 23:15:30.000000000 +1000
+++ torch/_inductor/runtime/triton_heuristics.py	2025-07-23 00:50:19.545650500 +1000
@@ -62,6 +62,7 @@
     Config,
     GPUTarget,
     KernelInterface,
+    knobs,
     OutOfResources,
     PTXASError,
     triton,
@@ -172,6 +173,8 @@
     configs, and does not rely on the Triton JIT.
     """
 
+    _warned_constexpr_args = set()
+
     def __init__(
         self,
         fn,
@@ -557,9 +560,21 @@
         if triton_version_uses_attrs_dict():
             # first: aggregate the constexpr args in (index, val) pairs
             # so we can sort them by index.
+
+            non_constexpr_args = []
             constexpr_args: list[tuple[int, Any]] = []
             for arg_name, arg_val in launcher.config.kwargs.items():
-                constexpr_args.append((self.fn.arg_names.index(arg_name), arg_val))
+                if arg_name in self.fn.arg_names:
+                    constexpr_args.append((self.fn.arg_names.index(arg_name), arg_val))
+                else:
+                    non_constexpr_args.append(arg_name)
+                    if arg_name not in self._warned_constexpr_args:
+                        print(f"\033[93m%%% [fix] ignoring triton_heuristics::_get_args_with_constexprs: missing arg_name: {arg_name}={arg_val}\033[0m -- torch/_inductor/runtime/triton_heuristics.py")
+                        #  print("\033[03m%%% [debug] self.fn.arg_names: {}\033[0m".format(str(self.fn.arg_names)))
+                        # see also triton/compiler/compiler.py
+                        # see also torch/_inductor/runtime/triton_heuristics.py
+                        # see also torch/_higher_order_ops/triton_kernel_wrap.py
+                        self._warned_constexpr_args.add(arg_name)
 
             constexpr_args.sort()
             new_args = [*args]
@@ -1147,11 +1162,18 @@
             binary.shared if hasattr(binary, "shared") else binary.metadata.shared
         )
 
+        if knobs is None:
+            launch_enter = binary.__class__.launch_enter_hook
+            launch_exit = binary.__class__.launch_exit_hook
+        else:
+            launch_enter = knobs.runtime.launch_enter_hook
+            launch_exit = knobs.runtime.launch_exit_hook
+
         scope = {
             "grid_meta": cfg.kwargs,
             "bin": binary,
-            "launch_enter_hook": binary.__class__.launch_enter_hook,
-            "launch_exit_hook": binary.__class__.launch_exit_hook,
+            "launch_enter_hook": launch_enter,
+            "launch_exit_hook": launch_exit,
             "metadata": (
                 binary.packed_metadata
                 if hasattr(binary, "packed_metadata")
@@ -1202,10 +1224,10 @@
             # `launch_enter_hook` is installed.  So if we don't have that hook installed,
             # we want to burn None in to the launch args with zero overhead.
             # See https://github.com/pytorch/pytorch/issues/123597
-            if binary.__class__.launch_enter_hook:
-                launch_metadata = f"bin.launch_metadata((grid_0, grid_1, grid_2), stream, {', '.join(call_args)})"
-            else:
-                launch_metadata = "None"
+            #  if binary.__class__.launch_enter_hook:
+            #      launch_metadata = f"bin.launch_metadata((grid_0, grid_1, grid_2), stream, {', '.join(call_args)})"
+            #  else:
+            launch_metadata = "None"
             runner_args = [
                 "grid_0",
                 "grid_1",
diff -ru --ignore-trailing-space /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/utils.py torch/_inductor/utils.py
--- /tmp/tmp.OOB7NJEiKv/unzipped/torch/_inductor/utils.py	2025-04-15 23:15:30.000000000 +1000
+++ torch/_inductor/utils.py	2025-07-22 23:46:10.878825700 +1000
@@ -1228,31 +1228,18 @@
 
 @functools.lru_cache(None)
 def is_big_gpu(index_or_device: Union[int, torch.device] = 0) -> bool:
-    if isinstance(index_or_device, torch.device):
-        device = index_or_device
-    else:
-        device = torch.device(get_gpu_type(), index_or_device)
+    arch = os.environ.get("TRITON_OVERRIDE_ARCH")
 
-    prop = DeviceProperties.create(device)
+    if arch is None:
+        print("\033[93m%%% [warn] No TRITON_OVERRIDE_ARCH set — no max_autotune_gemm for you until you set it.\033[0m")
+        return False
 
-    # SM logic is not relevant to ROCm gpus
-    # Arbitrarily skipping the older models
-    if torch.version.hip:
-        assert prop.major is not None
-        if prop.major < 9 or prop.major == 10:
-            log.warning("GPU arch does not support max_autotune_gemm mode usage")
-            return False
+    arch = arch.lower()
+    if arch.startswith("gfx11") or arch.startswith("gfx12"):
         return True
 
-    min_sms = 16 if device.type == "xpu" else 68  # 3080
-    avail_sms = prop.multi_processor_count
-    if avail_sms < min_sms:
-        log.warning(
-            "Not enough SMs to use max_autotune_gemm mode",
-            extra={"min_sms": min_sms, "avail_sms": avail_sms},
-        )
-        return False
-    return True
+    print("\033[93m%%% [warn] Your old Radeon is not supported for max_autotune_gemm mode.\033[0m")
+    return False
 
 
 @functools.lru_cache
@@ -1878,8 +1865,12 @@
 @functools.lru_cache(None)
 def get_gpu_dram_gbps() -> int:
     from triton.testing import get_dram_gbps
+    rv = get_dram_gbps()
 
-    return get_dram_gbps()
+    if not rv:
+        print(f"\033[93m%%% [fix] torch::_inductor::utils::get_gpu_dram_gps didn't know your dram speed, set to 20 (gfx1100)\033[0m ✔")
+        return 20
+    return rv
 
 
 def get_gpu_shared_memory() -> int:
