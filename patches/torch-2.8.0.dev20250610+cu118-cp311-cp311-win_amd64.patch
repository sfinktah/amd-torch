diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/backends/common.py torch/_dynamo/backends/common.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/backends/common.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/backends/common.py	2025-08-05 12:18:44.566456900 +1000
@@ -68,10 +68,7 @@
 
         def wrap_bw_compiler(bw_compiler_fn):
             def _wrapped_bw_compiler(*args, **kwargs):
-                # Note [Wrapping bw_compiler in disable]
-                # The two disables here:
-                # - stop TorchDynamo from trying to compile the bw_compiler function itself
-                # - stop TorchDynamo from trying to compile our the generated backwards pass bw_compiler produces
+                # stop TorchDynamo from trying to compile our generated backwards pass
                 return disable(
                     disable(
                         bw_compiler_fn, reason="do not trace backward compiler function"
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/exc.py torch/_dynamo/exc.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/exc.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/exc.py	2025-08-05 12:18:44.566456900 +1000
@@ -404,7 +404,6 @@
     torch._subclasses.fake_tensor.DynamicOutputShapeException,
     torch._subclasses.fake_tensor.UnsupportedOperatorException,
     torch._subclasses.fake_tensor.UnsupportedFakeTensorException,
-    torch._subclasses.fake_tensor.UnsupportedMutationAliasingException,
 )
 
 
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/side_effects.py torch/_dynamo/side_effects.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/side_effects.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/side_effects.py	2025-08-05 12:18:44.566456900 +1000
@@ -1,28 +1,5 @@
 # mypy: allow-untyped-defs
 
-"""
-Side effect tracking and management for TorchDynamo's compilation system.
-
-This module provides infrastructure for tracking and managing side effects that occur
-during symbolic execution, including:
-
-- Tracking mutations to objects, attributes, and variables
-- Managing context changes (cell variables, global namespace modifications)
-- Handling aliasing and object identity preservation
-- Managing stack frame state and local variable changes
-- Tracking function calls with side effects
-
-Key classes:
-- SideEffects: Main container for tracking all side effects during execution
-- MutableSideEffects: Specialization for mutable object tracking
-- AttributeMutation/ValueMutation: Track specific types of mutations
-- Various specialized side effect classes for different scenarios
-
-The side effect system ensures that mutations performed during symbolic execution
-are properly replayed during runtime, maintaining the correctness of compiled code
-while enabling optimizations where safe.
-"""
-
 import collections
 import contextlib
 import inspect
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/trace_rules.py torch/_dynamo/trace_rules.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/trace_rules.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/trace_rules.py	2025-08-05 12:18:44.566456900 +1000
@@ -1,26 +1,4 @@
 # mypy: allow-untyped-defs
-
-"""
-Tracing rules and policies for TorchDynamo compilation decisions.
-
-This module defines the rules that govern what code TorchDynamo should trace and compile
-versus what should be executed eagerly. It contains functions and classes that determine:
-
-- Which modules, functions, and objects should be skipped during tracing
-- Which parts of the code should cause graph breaks
-- How to handle different Python libraries and third-party packages
-- Rules for determining when to inline functions vs calling them eagerly
-
-Key components:
-- Skip rules: Functions that return True if an object should be skipped during tracing
-- Inlining rules: Policies for when to inline function calls during compilation
-- Library-specific handling: Special cases for popular Python packages
-- Performance heuristics: Rules that balance compilation overhead vs runtime benefits
-
-These rules are critical for TorchDynamo's ability to automatically determine
-compilation boundaries and optimize PyTorch programs effectively.
-"""
-
 import abc
 import builtins
 import collections
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/variables/builtin.py torch/_dynamo/variables/builtin.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/variables/builtin.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/variables/builtin.py	2025-08-05 12:18:44.582087700 +1000
@@ -1,26 +1,5 @@
 # mypy: allow-untyped-defs
 
-"""
-Built-in function and type variable tracking for TorchDynamo's symbolic execution.
-
-This module contains variable tracker classes for Python built-in functions, types,
-and operations during graph compilation. It handles symbolic execution of:
-
-- Built-in functions (len, getattr, isinstance, etc.)
-- Type constructors (int, float, str, list, dict, etc.)
-- Built-in operators and methods
-- Special Python constructs (super, hasattr, etc.)
-
-Key classes:
-- BuiltinVariable: Tracks built-in functions and handles their execution
-- TypeVariable: Manages type constructor calls and type checking
-- SuperVariable: Handles super() calls in class hierarchies
-
-These variable trackers ensure that built-in Python operations are correctly
-handled during symbolic execution, either by executing them directly when safe
-or by creating appropriate graph nodes when needed.
-"""
-
 import contextlib
 import functools
 import inspect
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/variables/tensor.py torch/_dynamo/variables/tensor.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_dynamo/variables/tensor.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_dynamo/variables/tensor.py	2025-08-05 12:18:44.597711500 +1000
@@ -45,6 +45,7 @@
 from .. import config, graph_break_hints, variables
 from .._trace_wrapped_higher_order_op import trace_wrapped
 from ..exc import (
+    unimplemented,
     unimplemented_v2,
     UnknownPropertiesDuringBackwardTrace,
     UserError,
@@ -375,12 +376,7 @@
             return ConstantVariable.create(self.is_nested)
 
     def method_attr_retain_grad(self, tx):
-        unimplemented_v2(
-            gb_type="Tensor.retain_grad() with AOTDispatcher",
-            context=f"var_getattr {self} retain_grad",
-            explanation="`Tensor.retain_grad()` does not work with AOTDispatcher.",
-            hints=[],
-        )
+        unimplemented("retain_grad does not work with AOTDispatcher")
 
     def method_attr_data(self, tx):
         return variables.TorchInGraphFunctionVariable(
@@ -389,12 +385,7 @@
 
     def method_attr_grad_fn(self, tx):
         if self.has_grad_fn:
-            unimplemented_v2(
-                gb_type="Tensor with grad_fn()",
-                context=f"var_getattr {self} grad_fn",
-                explanation="Dynamo does not support tracing tensors with a grad_fn directly.",
-                hints=[],
-            )
+            unimplemented("TensorVariable has a grad_fn")
         else:
             return variables.ConstantVariable(None)
 
@@ -436,14 +427,8 @@
     def var_getattr(self, tx: "InstructionTranslator", name):
         if self.is_strict_mode(tx):
             if name in self._strict_mode_banned_ops():
-                unimplemented_v2(
-                    gb_type="Strict mode banned op",
-                    context=f"var_getattr {self} {name}",
-                    explanation=f"Getattr invocation '{name}' in strict mode is not supported.",
-                    hints=[
-                        f"Remove `{name}` from the list of banned ops by "
-                        "setting `torch._dynamo.config._autograd_backward_strict_mode_banned_ops`.",
-                    ],
+                unimplemented(
+                    f"Getattr invocation {name} in strict mode is not supported"
                 )
             elif name in self._strict_mode_conditional_banned_ops():
                 raise UnknownPropertiesDuringBackwardTrace(
@@ -526,34 +511,17 @@
 
     def call_id(self, tx):
         if not self.source:
-            unimplemented_v2(
-                gb_type="Unsupported call_id() without source",
-                context=f"call_id {self}",
-                explanation="call_id() not supported for sourceless TensorVariable.",
-                hints=[],
-            )
+            unimplemented("call_id not supported for sourceless TensorVariable")
 
         # For local source, we associate the real value. We use this real value
         scope = {"L": tx.output.local_scope, "G": tx.output.global_scope}
         try:
             _input_associated_real_value = eval(self.source.name(), scope)
         except Exception as exc:
-            unimplemented_v2(
-                gb_type="Error getting associated real value",
-                context=f"call_id {self}",
-                explanation="Dynamo encountered an error while trying to "
-                "get the associated real value.",
-                hints=[],
-                from_exc=exc,
-            )
+            unimplemented(f"error getting associated real value: {exc}")
 
         if _input_associated_real_value is None:
-            unimplemented_v2(
-                gb_type="call_id() without associated real value",
-                context=f"call_id {self}",
-                explanation="Dynamo could not find an associated real value for the tensor.",
-                hints=[],
-            )
+            unimplemented("call_id without associated real value")
 
         install_guard(self.source.make_guard(GuardBuilder.ID_MATCH))
         id_value = id(_input_associated_real_value)
@@ -624,13 +592,7 @@
         from .torch_function import can_dispatch_torch_function, dispatch_torch_function
 
         if self.is_strict_mode(tx) and name in self._strict_mode_banned_ops():
-            unimplemented_v2(
-                gb_type="Illegal method invocation in strict mode",
-                context=f"call_method {self} {name} {args} {kwargs}",
-                explanation="Dynamo currently does not support this method "
-                f"({name}) invocation in strict mode.",
-                hints=[],
-            )
+            unimplemented(f"Illegal method invocation {name} in strict mode")
 
         # Only override builtin tensor methods
         # The user can manually add override handling
@@ -698,14 +660,7 @@
                 if result:
                     return result
             except TypeError as e:
-                unimplemented_v2(
-                    gb_type="Unhandled args for method",
-                    context=f"call_method {self} {name} {args} {kwargs}",
-                    explanation="Dynamo encountered an error while calling "
-                    f"the method `{name}`.",
-                    hints=[],
-                    from_exc=e,
-                )
+                unimplemented(f"unhandled args for {name}: {e}")
 
         from .builder import wrap_fx_proxy
 
@@ -895,26 +850,9 @@
 
     def method_numpy(self, *, force=False):
         if not config.trace_numpy:
-            unimplemented_v2(
-                gb_type="Tensor.numpy() with trace_numpy=False",
-                context=f"call_method {self} numpy",
-                explanation="`Tensor.numpy()` was called, but the `trace_numpy` "
-                "configuration was manually disabled.",
-                hints=[
-                    "Set `torch._dynamo.config.trace_numpy = True` to allow "
-                    "Dynamo to trace through NumPy.",
-                ],
-            )
+            unimplemented("Tensor.numpy(). config.trace_numpy is False")
         if not np:
-            unimplemented_v2(
-                gb_type="Tensor.numpy() without NumPy installed",
-                context=f"call_method {self} numpy",
-                explanation="`Tensor.numpy()` was called, but the NumPy library "
-                "is not available in the current environment.",
-                hints=[
-                    "Ensure NumPy is installed in your Python environment.",
-                ],
-            )
+            unimplemented("Tensor.numpy(). NumPy is not available")
         if self.layout != torch.strided:
             raise TypeError(
                 f"can't convert {self.layout} layout tensor to numpy. Use Tensor.to_dense() first"
@@ -960,16 +898,7 @@
                 torch.int32,
                 torch.int64,
             ]:
-                unimplemented_v2(
-                    gb_type="Tensor.tolist() with non-integer tensor",
-                    context=f"call_method {self} to_list",
-                    explanation="Dynamo currently does not support tracing "
-                    "`tolist()` on non-integer tensors.",
-                    hints=[
-                        "Ensure the input tensor to `tolist()` is an integer "
-                        "type (e.g., int8, int16, int32, int64)."
-                    ],
-                )
+                unimplemented("Input tensor for tolist must be an integer tensor")
 
             if tensor.dim() == 0:
                 return wrap(tensor, sub_proxy)
@@ -987,12 +916,7 @@
         return VariableTracker.build(tx, out)
 
     def method_backward(self, *args, **kwargs):
-        unimplemented_v2(
-            gb_type="Unsupported Tensor.backward() call",
-            context=f"call_method {self} backward {args} {kwargs}",
-            explanation="Dynamo currently does not support tracing `Tensor.backward()`.",
-            hints=[*graph_break_hints.FUNDAMENTAL],
-        )
+        unimplemented("Tensor.backward")
 
     def method_data_ptr(self, *args, **kwargs):
         return DataPtrVariable(self)
@@ -1000,17 +924,7 @@
     def method_item(self, *args, **kwargs):
         if not config.capture_scalar_outputs:
             self._warn_capture_scalar_outputs()
-            unimplemented_v2(
-                gb_type="Unsupported Tensor.item() call with capture_scalar_outputs=False",
-                context=f"call_method {self} item {args} {kwargs}",
-                explanation="Dynamo does not support tracing `Tensor.item()` "
-                "with config.capture_scalar_outputs=False.",
-                hints=[
-                    "Set `torch._dynamo.config.capture_scalar_outputs = True` "
-                    "or `export TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` "
-                    "to include these operations in the captured graph.",
-                ],
-            )
+            unimplemented("Tensor.item")
 
     def method___getitem__(self, *args, **kwargs):
         from ..symbolic_convert import InstructionTranslator
@@ -1099,36 +1013,16 @@
         return ConstantVariable.create(None)
 
     def method_resize_(self, *args, **kwargs):
-        unimplemented_v2(
-            gb_type="Unsupported Tensor.resize_() call",
-            context=f"call_method {self} resize_ {args} {kwargs}",
-            explanation="Dynamo currently does not support tracing `Tensor.resize_()`.",
-            hints=[],
-        )
+        unimplemented("Tensor.resize_")
 
     def method_resize_as_(self, *args, **kwargs):
-        unimplemented_v2(
-            gb_type="Unsupported Tensor.resize_as_() call",
-            context=f"call_method {self} resize_as_ {args} {kwargs}",
-            explanation="Dynamo currently does not support tracing `Tensor.resize_as_()`.",
-            hints=[],
-        )
+        unimplemented("Tensor.resize_as_")
 
     def method_sparse_resize_(self, *args, **kwargs):
-        unimplemented_v2(
-            gb_type="Unsupported Tensor.sparse_resize_() call",
-            context=f"call_method {self} sparse_resize_ {args} {kwargs}",
-            explanation="Dynamo currently does not support tracing `Tensor.sparse_resize_()`.",
-            hints=[],
-        )
+        unimplemented("Tensor.sparse_resize_")
 
     def method_sparse_resize_and_clear_(self, *args, **kwargs):
-        unimplemented_v2(
-            gb_type="Unsupported Tensor.sparse_resize_and_clear_() call",
-            context=f"call_method {self} sparse_resize_and_clear_ {args} {kwargs}",
-            explanation="Dynamo currently does not support tracing `Tensor.sparse_resize_and_clear_()`.",
-            hints=[],
-        )
+        unimplemented("Tensor.sparse_resize_and_clear_")
 
     def method_set_(self, *args, **kwargs):
         if len(args) > 1:
@@ -1138,13 +1032,7 @@
             # overload and is used by FSDP.
             # graph-breaking on aten::set_source_Tensor_storage_offset for now,
             # unless we find that we need to make it work.
-            unimplemented_v2(
-                gb_type="Unsupported Tensor.set_() call",
-                context=f"call_method {self} set_ {args} {kwargs}",
-                explanation="Dynamo currently does not support tracing `Tensor.set_()` "
-                "overloads that include more than one argument.",
-                hints=[*graph_break_hints.SUPPORTABLE],
-            )
+            unimplemented("Tensor.set_.source_Tensor_storage_offset")
 
     def method_add_(self, other, *, alpha=None):
         if alpha is not None:
@@ -1270,11 +1158,8 @@
                 # would have no recourse - their forward traces just fine, but will fail at backwards unless
                 # compiled_autograd is enabled. If compiled_autograd fails (there are a lot of failures today)
                 # then they have nothing they can do except disable compile.
-                unimplemented_v2(
-                    gb_type="Compilation of intermediate hooks requires compiled autograd",
-                    context=f"var_getattr {self} {name}",
-                    explanation="Dynamo must be in compiled_autograd to register hooks.",
-                    hints=[],
+                unimplemented(
+                    "Compilation of intermediate hooks requires compiled autograd"
                 )
 
             hook_name, bw_state_proxy = tx.output.add_backward_state_hook(hook)
@@ -1320,13 +1205,7 @@
             requires_grad = requires_grad.as_python_constant()
 
         if self.as_proxy().node.meta["example_value"].requires_grad != requires_grad:
-            unimplemented_v2(
-                gb_type="Unsupported Tensor.requires_grad_() call",
-                context=f"call_method {self} requires_grad_",
-                explanation="Dynamo does not support changes to a Tensor's "
-                "`requires_grad` through calling `requires_grad_()`.",
-                hints=[],
-            )
+            unimplemented("Tensor.requires_grad_")
         else:
             return self
 
@@ -1512,19 +1391,9 @@
                 return ConstantVariable.create(int(r))
             return insert_into_graph()
         elif name in ["base", "flags", "dtype"]:
-            unimplemented_v2(
-                gb_type="Unsupported ndarray attribute access",
-                context=f"var_getattr {self} {name}",
-                explanation=f"Dynamo currently does not support tracing `ndarray.{name}`.",
-                hints=[],
-            )
+            unimplemented(f"TODO: add support for ndarray.{name}")
         elif name in ["__version__"]:
-            unimplemented_v2(
-                gb_type="Unsupported ndarray.__version__ access",
-                context=f"var_getattr {self} {name}",
-                explanation=f"Dynamo currently does not support tracing `ndarray.{name}`.",
-                hints=[],
-            )
+            unimplemented("delegate np.__version__ to NumPy")
         if result is None:
             raise NotImplementedError
         return result
@@ -1551,12 +1420,7 @@
             # delegate back to TensorVariable
             return super().call_method(tx, name, args, kwargs)
         if name in ("tostring", "tobytes", "__delattr__"):
-            unimplemented_v2(
-                gb_type="Unsupported ndarray method call",
-                context=f"call_method {self} {name} {args} {kwargs}",
-                explanation=f"`ndarray.{name}()` is not modelled in `torch._numpy`.",
-                hints=[],
-            )
+            unimplemented(f"{name} is not modelled in torch._numpy")
         proxy = tx.output.create_proxy(
             "call_function",
             numpy_method_wrapper(name),
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/_aot_autograd/autograd_cache.py torch/_functorch/_aot_autograd/autograd_cache.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/_aot_autograd/autograd_cache.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_functorch/_aot_autograd/autograd_cache.py	2025-08-05 12:18:44.628961900 +1000
@@ -589,15 +589,6 @@
     def _is_backward(self) -> bool:
         return True
 
-    def post_compile(
-        self, result: CompiledFxGraph, fx_config: _CompileFxKwargs
-    ) -> CompiledFxGraph:
-        compiled_bw = super().post_compile(result, fx_config)
-        # See note [Wrapping bw_compiler in disable]
-        # This is done by _wrapped_bw_compiler in torch/_dynamo/backends/common.py
-        # But since on cache hit we do not call the bw_compiler, we need to reapply the disable
-        return torch._dynamo.disable(compiled_bw, reason="do not trace generated backwards pass")  # type: ignore[return-value]
-
 
 # Forward types don't have any extra parameters, so this is just a TypeAlias, in essence
 class BundledCompiledForward(CompiledFxGraphLoadable):
@@ -608,14 +599,7 @@
 class BundledCompiledBackward(
     GenericCompiledBackward[CompiledFxGraph], CompiledFxGraphLoadable
 ):
-    def post_compile(
-        self, result: CompiledFxGraph, fx_config: _CompileFxKwargs
-    ) -> CompiledFxGraph:
-        compiled_bw = super().post_compile(result, fx_config)
-        # See note [Wrapping bw_compiler in disable]
-        # This is done by _wrapped_bw_compiler in torch/_dynamo/backends/common.py
-        # But since on cache hit we do not call the bw_compiler, we need to reapply the disable
-        return torch._dynamo.disable(compiled_bw, reason="do not trace generated backwards pass")  # type: ignore[return-value]
+    pass
 
 
 TForward = TypeVar("TForward", bound=InductorOutput)
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/_aot_autograd/input_output_analysis.py torch/_functorch/_aot_autograd/input_output_analysis.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/_aot_autograd/input_output_analysis.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_functorch/_aot_autograd/input_output_analysis.py	2025-08-05 12:18:44.628961900 +1000
@@ -300,21 +300,6 @@
         ]
     )
 
-    if torch._inductor.config.is_fbcode():
-        if symbolic and num_aliases > 400:
-            from torch._subclasses.fake_tensor import (
-                UnsupportedMutationAliasingException,
-            )
-            from torch._utils_internal import justknobs_check
-
-            msg = f"Encountered {num_aliases} dynamic, aliased/mutated inputs, consider setting dynamic=False"
-
-            if justknobs_check(
-                "pytorch/compiler:aliased_inputs_with_mutation_and_dyn_shapes_killswitch",
-                False,
-            ):
-                raise UnsupportedMutationAliasingException(msg)
-
     with maybe_suppress_guards():
         aliased_fwd_inputs = [fwd_inputs[i] for i in aliased_input_indices]
         actual_aliased_indices = {
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/partitioners.py torch/_functorch/partitioners.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_functorch/partitioners.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_functorch/partitioners.py	2025-08-05 12:18:44.613336900 +1000
@@ -1141,11 +1141,6 @@
         return gm
 
     # Build the graph op-by-op by starting from the node all the way to the end
-    # copy_ can be not using tangents at all, we must copy it.
-    for node in list(gm.graph.nodes)[: order[first_node_in_bwd]]:
-        if node.op == "call_function" and node.target == torch.ops.aten.copy_.default:
-            insert_node_in_graph(node)
-
     for node in list(gm.graph.nodes)[order[first_node_in_bwd] :]:
         insert_node_in_graph(node)
 
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codecache.py torch/_inductor/codecache.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codecache.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/codecache.py	2025-08-05 12:18:44.647936300 +1000
@@ -51,6 +51,7 @@
 from torch._dynamo.exc import SkipFrame
 from torch._dynamo.utils import CompileEventLogger, counters, dynamo_timed
 from torch._inductor import config, exc, metrics
+from torch._inductor.codegen.common import custom_backend_passes
 from torch._inductor.codegen.cuda import cuda_env
 from torch._inductor.codegen.rocm.compile_command import (
     rocm_compile_command,
@@ -72,7 +73,11 @@
     normalize_path_separator,
 )
 from torch._inductor.cpu_vec_isa import pick_vec_isa
-from torch._inductor.custom_graph_pass import CustomGraphPass, CustomGraphPassType
+from torch._inductor.custom_graph_pass import (
+    CustomGraphModulePass,
+    CustomGraphPass,
+    CustomGraphPassType,
+)
 from torch._inductor.freezing_utils import has_frozen_params, is_frozen_param
 from torch._inductor.runtime.compile_tasks import _reload_python_module
 from torch._inductor.runtime.runtime_utils import cache_dir, default_cache_dir
@@ -891,12 +896,16 @@
             config.post_grad_custom_post_pass
         )
 
+        self.custom_backend_passes = tuple(
+            map(self._get_custom_pass_detail, custom_backend_passes.values())
+        )
+
     def _get_custom_pass_detail(
-        self, custom_pass: CustomGraphPassType
+        self, custom_pass: Union[CustomGraphPassType, CustomGraphModulePass]
     ) -> Optional[Any]:
         if not custom_pass:
             return None
-        assert isinstance(custom_pass, CustomGraphPass)
+        assert isinstance(custom_pass, (CustomGraphPass, CustomGraphModulePass))
         return custom_pass.uuid()
 
 
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/common.py torch/_inductor/codegen/common.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/common.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/codegen/common.py	2025-08-05 12:18:44.677391000 +1000
@@ -65,6 +65,7 @@
 
     from torch.fx import GraphModule
 
+    from ..custom_graph_pass import CustomGraphModulePass
     from ..ir import Buffer, ChoiceCaller, FixedLayout, IRNode
     from ..loop_body import LoopBody
     from ..scheduler import BaseScheduling, Scheduler, SchedulerNode
@@ -351,6 +352,7 @@
 
 
 device_op_overrides_dict: dict[str, DeviceOpOverrides] = {}
+custom_backend_passes: dict[str, Optional[CustomGraphModulePass]] = {}
 
 
 # The code generated by Inductor consists of two main parts: kernel code and wrapper code.
@@ -379,10 +381,12 @@
     device_scheduling: SchedulingConstructor,
     device_wrapper_codegen: WrapperConstructor,
     device_cpp_wrapper_codegen: Optional[WrapperConstructor] = None,
+    device_custom_pass: Optional[CustomGraphModulePass] = None,
 ) -> None:
     device_codegens[device] = DeviceCodegen(
         device_scheduling, device_wrapper_codegen, device_cpp_wrapper_codegen
     )
+    custom_backend_passes[device] = device_custom_pass
 
 
 class BackendFeature(Enum):
@@ -441,6 +445,10 @@
     return None
 
 
+def get_custom_backend_pass_for_device(device: str) -> Optional[CustomGraphModulePass]:
+    return custom_backend_passes[device] if device in custom_backend_passes else None
+
+
 @functools.lru_cache(None)
 def init_backend_registration() -> None:
     from .cpp import CppScheduling
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/cpp_wrapper_cpu.py torch/_inductor/codegen/cpp_wrapper_cpu.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/cpp_wrapper_cpu.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/codegen/cpp_wrapper_cpu.py	2025-08-05 12:18:44.685264800 +1000
@@ -7,11 +7,12 @@
 import sys
 import textwrap
 from itertools import chain, count
-from typing import Callable, Optional, Protocol, TYPE_CHECKING, Union
+from typing import Any, Callable, Optional, Protocol, TYPE_CHECKING, Union
 
 import sympy
 
 import torch
+import torch._higher_order_ops.torchbind
 import torch._inductor.async_compile  # noqa: F401 required to warm up AsyncCompile pools
 import torch._ops
 from torch._inductor.runtime.runtime_utils import dynamo_timed
@@ -38,6 +39,9 @@
 
     from ..graph import GraphLowering
 
+    # At most, the list nesting can go one layer deep.
+    _OUTPUT_ARGS_TYPE = list[Union[Optional[str], list[Optional[str]]]]
+
 
 class HasWriteLine(Protocol):
     def writeline(self, line: Union[LineContext, DeferredLineBase, str]) -> None: ...
@@ -1880,10 +1884,10 @@
 
     def generate_extern_kernel_args_decl_if_needed(
         self,
-        op_overload,
-        raw_args,
-        output_args: Optional[list[str]] = None,
-        raw_outputs: Optional[list[ir.Buffer]] = None,
+        op_overload: Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator],
+        raw_args: Sequence[Any],
+        output_args: _OUTPUT_ARGS_TYPE,
+        raw_outputs: Sequence[ir.Buffer],
     ):
         schema = None
         if isinstance(op_overload, torch._higher_order_ops.torchbind.CallTorchBind):
@@ -1891,6 +1895,7 @@
             method = raw_args[1]
             schema = op_overload.schema(obj, method)
         else:
+            assert isinstance(op_overload, torch._ops.OpOverload), type(op_overload)
             schema = op_overload._schema
         assert schema is not None
         arg_types = [x.real_type for x in schema.arguments]
@@ -1986,7 +1991,9 @@
                 else:
                     fill_args(arg, arg_type)
 
-        def fill_output_arg(arg, return_type, is_mutated_output: bool):
+        def fill_output_arg(
+            arg: str, return_type: torch.JitType, is_mutated_output: bool
+        ) -> None:
             if isinstance(return_type, torch.TensorType):
                 if not is_mutated_output:
                     self.writeline(f"AtenTensorHandle {arg}_handle;  // output buffer")
@@ -2021,8 +2028,9 @@
             # None output is supported, but Optional return types are not yet supported
             if output_arg is None:
                 continue
-            elif isinstance(output_arg, (list, tuple)):
+            elif isinstance(output_arg, list):
                 for out in output_arg:
+                    assert out is not None, out
                     fill_output_arg(
                         out,
                         torch.TensorType.get(),
@@ -2041,73 +2049,73 @@
         self,
         buf_name: str,
         python_kernel_name: str,
-        cpp_kernel_name: str,
-        codegen_args: list[str],
-        op_overload: Optional[torch._ops.OpOverload] = None,
-        raw_args=None,
-        outputs=None,
-    ):
-        def extract_output_name(out):
+        codegen_args: Sequence[str],
+        op_overload: Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator],
+        raw_args: Sequence[Any],
+        outputs: Sequence[ir.Buffer],
+    ) -> None:
+        """Generate a call to a kernel not contained in the C-shim.  This results in
+        different code paths for AOT Inductor vs cpp_wrapper Inductor mode."""
+
+        def extract_output_name(
+            out: Optional[Union[ir.Buffer, Sequence[ir.Buffer]]],
+        ) -> Union[Optional[str], _OUTPUT_ARGS_TYPE]:
             if out is None:
                 return None
-            elif isinstance(out, (ir.MultiOutput, ir._CollectiveKernel)):
+            if isinstance(out, (ir.MultiOutput, ir._CollectiveKernel)):
                 return out.get_name()
-            elif isinstance(out, ir.MutationOutput):
+            if isinstance(out, ir.MutationOutput):
                 mutated_buf_names = out.get_mutation_names()
                 assert (
                     isinstance(mutated_buf_names, list) and len(mutated_buf_names) == 1
                 ), "Expect only one mutated buffer in MutationOutput"
                 return mutated_buf_names[0]
-            elif isinstance(out, (list, tuple)):
-                return type(out)(extract_output_name(o) for o in out)
-            else:
-                raise AssertionError(f"Unexpected output: {type(out)}")
+            if isinstance(out, (list, tuple)):
+                return [extract_output_name(o) for o in out]  # type: ignore[misc]
+            raise AssertionError(f"Unexpected output: {type(out)}")
+
+        if isinstance(op_overload, torch._ops.HigherOrderOperator):
+            assert isinstance(
+                op_overload, torch._higher_order_ops.torchbind.CallTorchBind
+            ), type(op_overload)
+            assert len(raw_args) > 1
+            obj = raw_args[0]
+            method = raw_args[1]
+            return_schema = op_overload.schema(obj, method).returns
+        else:
+            return_schema = op_overload._schema.returns
 
         # output_args has the same pytree structure as outputs
-
-        return_schema = None
-        if op_overload:
-            if isinstance(op_overload, torch._higher_order_ops.torchbind.CallTorchBind):
-                assert raw_args is not None
-                assert len(raw_args) > 1
-                obj = raw_args[0]
-                method = raw_args[1]
-                return_schema = op_overload.schema(obj, method).returns
-            else:
-                return_schema = op_overload._schema.returns
-        if op_overload and not return_schema:
+        if not return_schema:
             # kernel does not return a value
-            output_args = []
-        elif outputs is None:
-            # outputs is not specified, the default is to write to buf_name
-            output_args = [buf_name]
-        else:
-            output_args = extract_output_name(outputs)
-            if isinstance(output_args, str):
-                output_args = [output_args]
+            output_args: _OUTPUT_ARGS_TYPE = []
+        elif isinstance(output_name := extract_output_name(outputs), str):
+            output_args = [output_name]
+        else:
+            # If the schema indicates a return value, we should have a non-None value by
+            # this point.
+            assert isinstance(output_name, list), type(output_name)
+            output_args = output_name
 
+        # In AOT mode, we use a ProxyExecutor to run fallback kernels.
         if V.graph.aot_mode:
-            assert op_overload is not None
-            assert raw_args is not None
-            assert output_args is not None
-
-            return self.generate_fallback_kernel_with_runtime_lookup_aot(
+            self.generate_fallback_kernel_with_runtime_lookup_aot(
                 op_overload,
                 raw_args,
                 output_args,
                 outputs,
             )
-        else:
-            return self.generate_fallback_kernel_with_runtime_lookup_jit(
-                buf_name,
-                python_kernel_name,
-                cpp_kernel_name,
-                codegen_args,
-                op_overload,
-                raw_args,
-                output_args,  # type: ignore[arg-type]
-                outputs,
-            )
+            return
+
+        assert isinstance(op_overload, torch._ops.OpOverload), type(op_overload)
+        self.generate_fallback_kernel_with_runtime_lookup_jit(
+            buf_name,
+            python_kernel_name,
+            op_overload,
+            raw_args,
+            output_args,  # type: ignore[arg-type]
+            outputs,
+        )
 
     def generate_scoped_gil_acquire(self, declarations_before_scope, lines_in_scope):
         scoped_lines = IndentedBuffer()
@@ -2256,19 +2264,19 @@
         self,
         buf_name: str,
         python_kernel_name: str,
-        cpp_kernel_name: str,
-        codegen_args: list[str],
-        op_overload: Optional[torch._ops.OpOverload] = None,
-        raw_args=None,
-        output_args: Optional[list[Optional[str]]] = None,
-        raw_outputs: Optional[list[ir.Buffer]] = None,
-    ):
-        # In the JIT mode, because of the ABI-compatible requirement, we can't directly call
-        # c10::Dispatcher to find the custom op and call it. Instead, we go back to Python
-        # to invoke this custom op.
+        op_overload: torch._ops.OpOverload,
+        raw_args: Sequence[Any],
+        output_args: Sequence[Optional[str]],
+        raw_outputs: Sequence[ir.Buffer],
+    ) -> None:
+        """Generate fallback kernel calls with runtime (non-AOT) dispatch.  This can
+        only be called in cpp_wrapper mode, and assumes that the input is a non-None
+        OpOverload.
+
+        This function calls into Python to dispatch, which allows it to handle datatypes
+        that cannot be contained in StableIValue, at the cost of some performance."""
         self.load_custom_op_wrapper()
 
-        assert output_args is not None, "output_args should not be None"
         num_args = len(raw_args)
         py_args_var = f"py_args_{next(self.arg_var_id)}"
         # First arg is always the python op name
@@ -2282,8 +2290,6 @@
             """
         )
 
-        assert op_overload is not None, "op_overload should not be None"
-
         for idx, (raw_arg, schema_arg) in enumerate(
             zip(raw_args, op_overload._schema.arguments)
         ):
@@ -2334,11 +2340,11 @@
 
     def generate_fallback_kernel_with_runtime_lookup_aot(
         self,
-        op_overload,
-        raw_args,  # contains both args and flatten kwargs
-        output_args: Optional[list[str]] = None,
-        raw_outputs: Optional[list[ir.Buffer]] = None,
-    ):
+        op_overload: Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator],
+        raw_args: Sequence[Any],
+        output_args: _OUTPUT_ARGS_TYPE,
+        raw_outputs: Sequence[ir.Buffer],
+    ) -> None:
         (
             tensor_call_args,
             int_call_args,
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/codegen/cpp_wrapper_cpu_array_ref.py	2025-08-05 12:18:44.685264800 +1000
@@ -1,5 +1,6 @@
 # mypy: allow-untyped-defs
-from typing import Callable, Optional
+from collections.abc import Sequence
+from typing import Any, Callable, Optional, Union
 
 import sympy
 
@@ -749,57 +750,16 @@
         self,
         buf_name: str,
         python_kernel_name: str,
-        cpp_kernel_name: str,
-        codegen_args: list[str],
-        op_overload: Optional[torch._ops.OpOverload] = None,
-        raw_args=None,
-        outputs=None,
-    ):
+        codegen_args: Sequence[str],
+        op_overload: Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator],
+        raw_args: Sequence[Any],
+        outputs: Sequence[ir.Buffer],
+    ) -> None:
         # No stack allocation when there is a fallback op
         self.allow_stack_allocation = False
-
-        def extract_output_name(out):
-            if out is None:
-                return None
-            elif isinstance(out, (ir.MultiOutput, ir._CollectiveKernel)):
-                return out.get_name()
-            elif isinstance(out, (list, tuple)):
-                return type(out)(extract_output_name(o) for o in out)
-            else:
-                raise AssertionError(f"Unexpected output: {type(out)}")
-
-        # output_args has the same pytree structure as outputs
-        output_args = None
-        if outputs is None:
-            # outputs is not specified, the default is to write to buf_name
-            output_args = [buf_name]
-        else:
-            output_args = extract_output_name(outputs)
-            if isinstance(output_args, str):
-                output_args = [output_args]
-
-        if V.graph.aot_mode:
-            assert op_overload is not None
-            assert raw_args is not None
-            assert outputs is not None
-
-            return self.generate_fallback_kernel_with_runtime_lookup_aot(
-                op_overload,
-                raw_args,
-                output_args,
-                outputs,
-            )
-        else:
-            return self.generate_fallback_kernel_with_runtime_lookup_jit(
-                buf_name,
-                python_kernel_name,
-                cpp_kernel_name,
-                codegen_args,
-                op_overload,
-                raw_args,
-                output_args,  # type: ignore[arg-type]
-                outputs,
-            )
+        super().generate_fallback_kernel_with_runtime_lookup(
+            buf_name, python_kernel_name, codegen_args, op_overload, raw_args, outputs
+        )
 
     def codegen_device_copy(self, src, dst, non_blocking: bool):
         # aoti_torch_tensor_copy_ takes AtenTensorHandle as input,
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/wrapper.py torch/_inductor/codegen/wrapper.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/codegen/wrapper.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/codegen/wrapper.py	2025-08-05 12:18:44.692680100 +1000
@@ -1417,12 +1417,11 @@
         self,
         buf_name: str,
         python_kernel_name: str,
-        cpp_kernel_name: str,
-        codegen_args: list[str],
-        op_overload: Optional[torch._ops.OpOverload] = None,
-        raw_args=None,
-        outputs=None,
-    ):
+        codegen_args: Sequence[str],
+        op_overload: Union[torch._ops.OpOverload, torch._ops.HigherOrderOperator],
+        raw_args: Sequence[Any],
+        outputs: Sequence[ir.Buffer],
+    ) -> None:
         self.writeline(f"{buf_name} = {python_kernel_name}({', '.join(codegen_args)})")
 
     def generate(self, is_inference):
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/compile_fx.py torch/_inductor/compile_fx.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/compile_fx.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/compile_fx.py	2025-08-05 12:18:44.647936300 +1000
@@ -76,6 +76,7 @@
     BoxedBool,
     count_tangents,
     fresh_inductor_cache,
+    get_all_devices,
     InputType,
     is_gpu,
     should_assume_input_aligned,
@@ -1909,22 +1910,6 @@
     }
 
 
-def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]:
-    placeholder_nodes = gm.graph.find_nodes(op="placeholder")
-    input_devices: OrderedSet[torch.device] = OrderedSet(
-        node.meta["val"].device
-        for node in placeholder_nodes
-        if isinstance(node.meta.get("val"), torch.Tensor)
-    )
-
-    out_devices: OrderedSet[torch.device] = OrderedSet(
-        arg.meta["val"].device
-        for arg in output_node(gm).args[0]  # type: ignore[union-attr]
-        if isinstance(arg, fx.Node) and isinstance(arg.meta.get("val"), torch.Tensor)
-    )
-    return input_devices | out_devices
-
-
 def get_cuda_device_context(gm: torch.fx.GraphModule) -> AbstractContextManager[None]:
     """
     Returns a cuda device context manager if there is a single device in the graph
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/custom_graph_pass.py torch/_inductor/custom_graph_pass.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/custom_graph_pass.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/custom_graph_pass.py	2025-08-05 12:18:44.647936300 +1000
@@ -53,6 +53,38 @@
         """
 
 
+class CustomGraphModulePass(ABC):
+    """
+    Implement this interface for custom Graph passes:
+
+    1) The __call__() method contains the implementation of the custom pass.
+
+    2) The uuid() method enables inductor to cache compiled graphs when your custom
+    passes are applied. This method can return any identifier as long as it uniquely
+    identifies your implementation (and can be pickled). The caching logic includes this
+    identifier in its key calculation, i.e., any new value will effectively invalidate
+    existing entries. We expect custom passes would typically depend purely on the
+    textual reprensentation of the implementation. In that case, we recommend using the
+    'get_hash_for_files' helper below to compute a unique hash from the contents of a
+    static list of source files, i.e., the source(s) containing the custom pass
+    implementation. That approach ensures that any change to the implementation will
+    mean a new uuid.
+    """
+
+    @abstractmethod
+    def __call__(self, gm: torch.fx.GraphModule) -> None:
+        """
+        Implementation of the custom pass.
+        """
+
+    @abstractmethod
+    def uuid(self) -> Optional[Any]:
+        """
+        Return an ID to uniquely identify your custom pass implementation. Return None
+        to skip inductor code caching entirely.
+        """
+
+
 CustomGraphPassType: TypeAlias = Optional[
     Union[CustomGraphPass, Callable[[torch.fx.graph.Graph], None]]
 ]
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/fx_passes/post_grad.py torch/_inductor/fx_passes/post_grad.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/fx_passes/post_grad.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/fx_passes/post_grad.py	2025-08-05 12:18:44.708184700 +1000
@@ -22,6 +22,7 @@
 from torch.utils._ordered_set import OrderedSet
 
 from .. import config, ir, pattern_matcher
+from ..codegen.common import custom_backend_passes
 from ..comms import remove_fsdp2_unsharded_param_graph_input_usage
 from ..fx_utils import FakeTensorUpdater, get_fake_args_kwargs, get_node_storage
 from ..lowering import lowerings as L
@@ -48,6 +49,7 @@
 )
 from ..utils import (
     decode_device,
+    get_all_devices,
     get_gpu_type,
     is_gpu,
     is_pointwise_use,
@@ -182,6 +184,13 @@
 
     fake_tensor_updater.incremental_update()
 
+    for device, custom_backend_pass in custom_backend_passes.items():
+        if custom_backend_pass is not None:
+            gm_devices = [d.type for d in get_all_devices(gm)]
+            if device in gm_devices:
+                pass_name = "custom_backend_passes_" + device
+                GraphTransformObserver(gm, pass_name).apply_gm_pass(custom_backend_pass)
+
     # Keep these last, since they introduces mutation. Look at
     # ./fx_passes/README.md for a discussion of mutation invariants.
     GraphTransformObserver(gm, "reinplace_inplaceable_ops").apply_graph_pass(
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/graph.py torch/_inductor/graph.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/graph.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/graph.py	2025-08-05 12:18:44.660448500 +1000
@@ -217,6 +217,7 @@
             aten.convolution,
             aten.convolution_backward,
             aten._scaled_mm,
+            aten._scaled_grouped_mm,
         ]
     )
     # what's a better way to collect the reduction ops?
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/ir.py torch/_inductor/ir.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/ir.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/ir.py	2025-08-05 12:18:44.660448500 +1000
@@ -3160,7 +3160,7 @@
 
     __repr__ = __str__
 
-    def get_name(self):  # type: ignore[no-untyped-def]
+    def get_name(self) -> str:
         return self.data.get_name()
 
     def get_device(self) -> Optional[torch.device]:
@@ -5186,6 +5186,8 @@
             self.schema_kwargs = [
                 x for x in self.op_overload._schema.arguments if x.kwarg_only
             ]
+        else:
+            self.schema_kwargs = []
 
     def decide_layout(self):  # type: ignore[no-untyped-def]
         if isinstance(self.layout, FlexibleLayout):
@@ -6036,7 +6038,7 @@
     def get_defining_op(self) -> Operation:
         return self.mutating_node
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return self.mutation_names
 
     def should_allocate(self) -> bool:
@@ -6371,7 +6373,7 @@
     def should_allocate(self) -> bool:
         return False
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[0].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -6403,7 +6405,7 @@
     def should_allocate(self) -> bool:
         return False
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[0].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -6456,7 +6458,7 @@
     def should_allocate(self) -> bool:
         return False
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[0].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -6538,7 +6540,7 @@
     def should_allocate(self) -> bool:
         return False
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[0].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -6602,7 +6604,7 @@
     def should_allocate(self) -> bool:
         return False
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[0].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -6945,7 +6947,7 @@
     def get_inputs_that_alias_output(self):  # type: ignore[no-untyped-def]
         return self.alias_names
 
-    def get_mutation_names(self):  # type: ignore[no-untyped-def]
+    def get_mutation_names(self) -> Sequence[str]:
         assert len(self.mutation_names) <= 1
         return self.mutation_names
 
@@ -7084,51 +7086,41 @@
                 kernel not in config.aot_inductor.custom_ops_to_c_shims
             )
 
-        def do_runtime_dispatch() -> None:
-            args = None
-            exported_args = self.export_extern_kernel_node()
+        def is_number(t: torch.JitType) -> bool:
+            if isinstance(t, torch.OptionalType):
+                return is_number(t.getElementType())
+            return isinstance(t, torch.NumberType)
 
+        self.codegen_comment(wrapper)
+        args = [*self.codegen_args(), *self.codegen_kwargs()]
+        if self.use_runtime_dispatch or (
+            # Handle the special case where a complex number is input to a
+            # cpp_wrapper C-shim kernel.  If the corresponding argument is a number,
+            # the torchgen-created shim API will use type "double", which cannot be
+            # converted to from a c10::complex.  In these cases, fallback to runtime
+            # dispatch.
+            V.graph.cpp_wrapper
+            and isinstance(kernel, torch._ops.OpOverload)
+            and any(
+                "c10::complex" in arg_str and is_number(op_arg.real_type)
+                for arg_str, op_arg in zip(args, kernel._schema.arguments)
+            )
+        ):
+            exported_args = self.export_extern_kernel_node()
             wrapper.generate_fallback_kernel_with_runtime_lookup(
                 self.get_name(),
                 self.python_kernel_name,
-                self.cpp_kernel_name,
                 args,
                 self.op_overload,
                 exported_args,
                 # NOTE: [special handling of all_reduce_coalesced_'s return value]
                 self.outputs if self.outputs else self.mutation_outputs,
             )
-
-        def is_number(t: torch.JitType) -> bool:
-            return isinstance(t, torch.NumberType) or (
-                isinstance(t, torch.OptionalType)
-                and isinstance(t.getElementType(), torch.NumberType)
-            )
-
-        self.codegen_comment(wrapper)
-        if self.use_runtime_dispatch:
-            do_runtime_dispatch()
         else:
-            args = [*self.codegen_args(), *self.codegen_kwargs()]
-            if (
-                V.graph.cpp_wrapper
-                and isinstance(kernel, torch._ops.OpOverload)
-                and any(
-                    "c10::complex" in arg_str and is_number(op_arg.real_type)
-                    for arg_str, op_arg in zip(args, kernel._schema.arguments)
-                )
-            ):
-                # Handle the special case where a complex number is input to a
-                # cpp_wrapper C-shim kernel.  If the corresponding argument is a number,
-                # the torchgen-created shim API will use type "double", which cannot be
-                # converted to from a c10::complex.  In these cases, fallback to runtime
-                # dispatch.
-                do_runtime_dispatch()
-            else:
-                wrapper.generate_fallback_kernel(self)
-                if isinstance(self.layout, Layout):
-                    self.codegen_size_asserts(wrapper)
-                    self.codegen_alignment_asserts(wrapper)
+            wrapper.generate_fallback_kernel(self)
+            if isinstance(self.layout, Layout):
+                self.codegen_size_asserts(wrapper)
+                self.codegen_alignment_asserts(wrapper)
 
         self.codegen_unbacked_symbol_defs(wrapper)
 
@@ -8087,7 +8079,7 @@
     name: str
     value: Union[FakeScriptObject, torch.ScriptObject]
 
-    def get_name(self):  # type: ignore[no-untyped-def]
+    def get_name(self) -> str:
         return self.name
 
     def codegen_reference(self, writer: Optional[IndentedBuffer] = None) -> str:
@@ -8120,7 +8112,7 @@
     name: str
     device: torch.device
 
-    def get_name(self):  # type: ignore[no-untyped-def]
+    def get_name(self) -> str:
         return self.name
 
     def codegen_reference(self, writer: Optional[IndentedBuffer] = None) -> str:
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/kernel/mm_common.py torch/_inductor/kernel/mm_common.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/kernel/mm_common.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/kernel/mm_common.py	2025-08-05 12:18:44.723815300 +1000
@@ -38,8 +38,7 @@
 
 
 @SymbolicGridFn
-def persistent_grouped_mm_grid(*args):
-    meta = args[-1]
+def persistent_grouped_mm_grid(m, n, meta):
     return (meta["NUM_SMS"], 1, 1)
 
 
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/kernel/mm_scaled_grouped.py torch/_inductor/kernel/mm_scaled_grouped.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/kernel/mm_scaled_grouped.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/kernel/mm_scaled_grouped.py	2025-08-05 12:18:44.723815300 +1000
@@ -10,6 +10,7 @@
 
 from ..ir import ChoiceCaller, Layout, TensorBox
 from ..lowering import register_lowering
+from ..runtime.runtime_utils import next_power_of_2
 from ..select_algorithm import (
     autotune_select_algorithm,
     ExternKernelChoice,
@@ -19,7 +20,7 @@
 from ..utils import (
     get_gpu_shared_memory,
     get_num_sms,
-    has_free_symbols,
+    get_tma_workspace_arg,
     use_aten_gemm_kernels,
 )
 from .mm_common import (
@@ -51,7 +52,7 @@
         num_stages=num_stages,
         num_warps=num_warps,
     )
-    for block_size_m in [16, 32, 64, 128]
+    for block_size_m in [64, 128]
     for block_size_n in [64, 128, 256]
     for block_size_k in [64, 128, 256]
     for num_stages in [3, 4]
@@ -80,11 +81,11 @@
 ]
 
 
-def grouped_mm_configs():
+def scaled_grouped_mm_configs():
     return _AMD_CONFIGS if torch.version.hip else _NV_CONFIGS
 
 
-def early_config_prune(g, m, configs, named_args):
+def early_config_prune(configs, named_args):
     dtsize = 1
     pruned_configs = []
     for config in configs:
@@ -97,26 +98,14 @@
             config.num_warps,
             getattr(config, "num_consumer_groups", 0),
         )
+        G, M, N, K = (
+            named_args["G"],
+            named_args["M_BUCKET"],
+            named_args["N"],
+            named_args["K"],
+        )
 
-        # 1. Prune NV configs depending on g and m.
-        if not torch.version.hip:
-            if not has_free_symbols((g, m)):
-                a_is_2d, b_is_2d = named_args["A_IS_2D"], named_args["B_IS_2D"]
-                m_avg = m // g if a_is_2d and not b_is_2d else m
-                if m_avg <= 16:
-                    if BLOCK_M > 32:
-                        continue
-                elif m_avg <= 32:
-                    if BLOCK_M > 64:
-                        continue
-                elif m_avg <= 64:
-                    if BLOCK_M <= 16:
-                        continue
-                else:
-                    if BLOCK_M <= 32:
-                        continue
-
-        # 2. make sure we have enough smem
+        # 1. make sure we have enough smem
         max_shared_memory = get_gpu_shared_memory()
 
         if torch.version.hip:
@@ -128,7 +117,39 @@
 
         use_warp_specialization = num_consumer_groups >= 1
 
-        # 3. make sure we can partition for ws
+        M_PER_GROUP = M // G
+        MIN_M_TILES = 32 if torch.version.hip else 64
+        # 2. make sure we don't load M tiles that are too big
+        if (
+            not use_warp_specialization
+            and BLOCK_M > MIN_M_TILES
+            and BLOCK_M > (M_PER_GROUP * 2)
+        ):
+            continue
+        # 3. make sure we don't load N tiles that are too small
+        if BLOCK_M < 128 and BLOCK_M < (M_PER_GROUP // 2):
+            continue
+
+        num_sm = get_num_sms()
+
+        N_TILES = N // BLOCK_N
+        MIN_N_TILES = 32 if torch.version.hip else 64
+        # 4. make sure we don't load N tiles that are too big
+        if (
+            not use_warp_specialization
+            and BLOCK_N > MIN_N_TILES
+            and M * N_TILES < num_sm
+        ):
+            continue
+        # 5. make sure we don't load N tiles that are too small
+        if BLOCK_N < 128 and M * N_TILES > 2 * num_sm:
+            continue
+
+        # 6. make sure K can be evenly divided
+        if K % BLOCK_K != 0:
+            continue
+
+        # 7. make sure we can partition for ws
         if use_warp_specialization:
             if num_warps != 4:
                 continue
@@ -145,129 +166,47 @@
 
 
 # Copied from fbgemm grouped_gemm.py
-triton_grouped_mm_source = r"""
-{%- if A_IS_2D or B_IS_2D %}
-{{def_kernel("a_ptr", "b_ptr", "scale_a_ptr", "scale_b_ptr", "offsets_ptr")}}
-{%- else %}
-{{def_kernel("a_ptr", "b_ptr", "scale_a_ptr", "scale_b_ptr")}}
-{%- endif %}
+triton_scaled_grouped_mm_source = r"""
+{{def_kernel("a_ptr", "b_ptr", "a_scale_ptr", "b_scale_ptr", "m_sizes")}}
     tidx = tl.program_id(0)
 
-{%- set M_IS_VARYING = A_IS_2D and not B_IS_2D %}
-{%- set N_IS_VARYING = not A_IS_2D and B_IS_2D %}
-{%- set K_IS_VARYING = A_IS_2D and B_IS_2D %}
-
-{%- if A_IS_2D %}
-{%- if B_IS_2D %}
-    G = {{size("offsets_ptr", 0)}}
-{%- else %}
-    G = {{size("b_ptr", 0)}}
-{%- endif %}
-{%- else %}
-{%- if B_IS_2D %}
-    G = {{size("a_ptr", 0)}}
-{%- else %}
-    G = {{size("a_ptr", 0)}}
-{%- endif %}
-{%- endif %}
-
-    # the b_ptr tensor is given with its last two dims transposed, revert here
-
-    M = {{size("a_ptr", -2)}}
-    N = {{size("b_ptr", -1)}}
-    K = {{size("a_ptr", -1)}}
-
-    A_STRIDE_M = {{stride("a_ptr", -2)}}
-    A_STRIDE_K = {{stride("a_ptr", -1)}}
-{%- if not A_IS_2D %}
-    A_STRIDE_G = {{stride("a_ptr", 0)}}
-    SCALE_A_STRIDE_G = {{stride("scale_a_ptr", 0)}}
-{%- endif %}
-    B_STRIDE_N = {{stride("b_ptr", -1)}}
-    B_STRIDE_K = {{stride("b_ptr", -2)}}
-{%- if not B_IS_2D %}
-    B_STRIDE_G = {{stride("b_ptr", 0)}}
-    SCALE_B_STRIDE_G = {{stride("scale_b_ptr", 0)}}
-{%- endif %}
-
-    # fixme: a_desc = tl.make_tensor_descriptor(
-    a_desc = tl._experimental_make_tensor_descriptor(
-        a_ptr,
-{%- if A_IS_2D %}
-        shape=[M, K],
-        # fixme: strides=[A_STRIDE_M, A_STRIDE_K],
-        strides=[{{stride("a_ptr", -2)}}, {{stride("a_ptr", -1)}}],
-        block_shape=[BLOCK_M, BLOCK_K],
-{%- else %}
-        shape=[G, M, K],
-        # fixme: strides=[A_STRIDE_G, A_STRIDE_M, A_STRIDE_K],
-        strides=[{{stride("a_ptr", 0)}}, {{stride("a_ptr", -2)}}, {{stride("a_ptr", -1)}}],
-        block_shape=[1, BLOCK_M, BLOCK_K],
-{%- endif %}
-    )
-
-    # fixme: b_desc = tl.make_tensor_descriptor(
-    b_desc = tl._experimental_make_tensor_descriptor(
-        b_ptr,
-{%- if B_IS_2D %}
-        shape=[N, K],
-        # fixme: strides=[B_STRIDE_N, B_STRIDE_K],
-        strides=[{{stride("b_ptr", -1)}}, {{stride("b_ptr", -2)}}],
-        block_shape=[BLOCK_N, BLOCK_K],
-{%- else %}
-        shape=[G, N, K],
-        # fixme: strides=[B_STRIDE_G, B_STRIDE_N, B_STRIDE_K],
-        strides=[{{stride("b_ptr", 0)}}, {{stride("b_ptr", -1)}}, {{stride("b_ptr", -2)}}],
-        block_shape=[1, BLOCK_N, BLOCK_K],
-{%- endif %}
-    )
-
-{%- if M_IS_VARYING %}
-    m_end_offset = 0
-{%- endif %}
-{%- if N_IS_VARYING %}
-    n_end_offset = 0
-{%- endif %}
-{%- if K_IS_VARYING %}
-    k_end_offset = 0
-{%- endif %}
+    dtype = tl.float8e4nv
+    TMA_SIZE: tl.constexpr = tl.constexpr(128)
+
+    workspace_base = ws_ptr + tidx * 2 * TMA_SIZE
+    c_desc_ptr = None
+
+    a_desc_ptr = workspace_base
+    b_desc_ptr = workspace_base + TMA_SIZE
+
+    triton.language.extra.cuda.experimental_device_tensormap_create2d(
+        desc_ptr=a_desc_ptr,
+        global_address=a_ptr,
+        load_size=[BLOCK_M, BLOCK_K],
+        global_size=[M, K],
+        element_ty=a_ptr.dtype.element_ty,
+    )
+    triton.language.extra.cuda.experimental_device_tensormap_create2d(
+        desc_ptr=b_desc_ptr,
+        global_address=b_ptr,
+        load_size=[BLOCK_N, BLOCK_K],
+        global_size=[N * G, K],
+        element_ty=b_ptr.dtype.element_ty,
+    )
+    tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(a_desc_ptr)
+    tl.extra.cuda.experimental_tensormap_fenceproxy_acquire(b_desc_ptr)
+
+    M_end_offset = 0
     iterated_tiles = 0
     for g in tl.range(G):
-{%- if M_IS_VARYING %}
-        # Move across groups
-        m_start_offset = m_end_offset
-        m_end_offset = tl.load(offsets_ptr + g)
-        m_size = m_end_offset - m_start_offset
-        m_scale_start_offset = m_start_offset
-{%- else %}
-        m_start_offset = 0
-        m_size = M
-        m_scale_start_offset = g * M
-{%- endif %}
-
-{%- if N_IS_VARYING %}
         # Move across groups
-        n_start_offset = n_end_offset
-        n_end_offset = tl.load(offsets_ptr + g)
-        n_size = n_end_offset - n_start_offset
-        n_scale_start_offset = n_start_offset
-{%- else %}
-        n_start_offset = 0
-        n_size = N
-        n_scale_start_offset = g * N
-{%- endif %}
-
-        if m_size > 0 and n_size > 0:
-{%- if K_IS_VARYING %}
-            # Move across groups
-            k_start_offset = k_end_offset
-            k_end_offset = tl.load(offsets_ptr + g)
-            k_size = k_end_offset - k_start_offset
-{%- else %}
-            k_start_offset = 0
-            k_size = K
-{%- endif %}
-
+        M_start_offset = M_end_offset
+        M_end_offset = tl.load(m_sizes + g)
+        m_size = M_end_offset - M_start_offset
+
+        if m_size > 0:
+            N_start_offset = g.to(tl.int64) * N
+            n_size = N
             num_m_tiles = tl.cdiv(m_size, BLOCK_M)
             num_n_tiles = tl.cdiv(n_size, BLOCK_N)
             num_tiles = num_m_tiles * num_n_tiles
@@ -280,111 +219,64 @@
                 tile_n_idx = gidx // num_m_tiles
 
                 accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
-
-{%- if USE_TMA_LOAD %}
-                m_offset = (m_start_offset + tile_m_idx * BLOCK_M).to(tl.int32)
-                n_offset = (n_start_offset + tile_n_idx * BLOCK_N).to(tl.int32)
-
-                for k_offset in range(0, k_size, BLOCK_K):
-{%- if A_IS_2D %}
-                    a = a_desc.load([m_offset, k_start_offset + k_offset])
-{%- else %}
-                    a = a_desc.load([g, m_offset, k_start_offset + k_offset]).reshape(BLOCK_M, BLOCK_K)
-{%- endif %}
-{%- if B_IS_2D %}
-                    b = b_desc.load([n_offset, k_start_offset + k_offset])
-{%- else %}
-                    b = b_desc.load([g, n_offset, k_start_offset + k_offset]).reshape(BLOCK_N, BLOCK_K)
-{%- endif %}
-
-{%- if K_IS_VARYING %}
-                    if k_offset + BLOCK_K > k_size:
-                        group_offs_k = k_offset + tl.arange(0, BLOCK_K)
-                        a = tl.where(group_offs_k < k_size, a, 0)
-                        b = tl.where(group_offs_k < k_size, b, 0)
-{%- endif %}
-
-{%- if USE_FAST_ACCUM %}
-                    accumulator = tl.dot(a, b.T, accumulator)
-{%- else %}
-                    accumulator += tl.dot(a, b.T)
-{%- endif %}
-{%- else %}
-                offs_am = tile_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)
-                offs_bn = tile_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)
-                offs_k = k_start_offset + tl.arange(0, BLOCK_K)
-                a_ptrs = (
-                    a_ptr
-{%- if not A_IS_2D %}
-                    + g * A_STRIDE_G
-{%- endif %}
-                    + (m_start_offset + offs_am[:, None]) * A_STRIDE_M
-                    + offs_k[None, :] * A_STRIDE_K
-                )
-                b_ptrs = (
-                    b_ptr
-{%- if not B_IS_2D %}
-                    + g * B_STRIDE_G
-{%- endif %}
-                    + (n_start_offset + offs_bn[:, None]) * B_STRIDE_N
-                    + offs_k[None, :] * B_STRIDE_K
-                )
-                for k_offset in range(0, k_size, BLOCK_K):
-                    a = tl.load(a_ptrs, mask=offs_am[:, None] < m_size)
-                    b = tl.load(b_ptrs, mask=offs_bn[:, None] < n_size)
-                    if k_offset + BLOCK_K > k_size:
-                        group_offs_k = k_offset + tl.arange(0, BLOCK_K)
-                        a = tl.where(group_offs_k < k_size, a, 0)
-                        b = tl.where(group_offs_k < k_size, b, 0)
-{%- if USE_FAST_ACCUM %}
-                    accumulator = tl.dot(a, b.T, accumulator)
-{%- else %}
-                    accumulator += tl.dot(a, b.T)
-{%- endif %}
-                    a_ptrs += BLOCK_K
-                    b_ptrs += BLOCK_K
-{%- endif %}
+                tl.static_assert(K % BLOCK_K == 0)
+                if USE_TMA_LOAD:
+                    m_offset = (M_start_offset + tile_m_idx * BLOCK_M).to(tl.int32)
+                    n_offset = (N_start_offset + tile_n_idx * BLOCK_N).to(tl.int32)
+                    for k_offset in range(0, K, BLOCK_K):
+                        a = tl._experimental_descriptor_load(
+                            a_desc_ptr,
+                            [m_offset, k_offset],
+                            [BLOCK_M, BLOCK_K],
+                            dtype,
+                        )
+                        b = tl._experimental_descriptor_load(
+                            b_desc_ptr,
+                            [n_offset, k_offset],
+                            [BLOCK_N, BLOCK_K],
+                            dtype,
+                        )
+                        if USE_FAST_ACCUM:
+                            accumulator = tl.dot(a, b.T, accumulator)
+                        else:
+                            accumulator += tl.dot(a, b.T)
+                else:
+                    offs_am = tile_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)
+                    offs_bn = tile_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)
+                    offs_k = tl.arange(0, BLOCK_K)
+                    a_ptrs = (
+                        a_desc_ptr
+                        + (M_start_offset + offs_am[:, None]) * K
+                        + offs_k[None, :]
+                    )
+                    b_ptrs = (
+                        b_desc_ptr
+                        + (N_start_offset + offs_bn[:, None]) * K
+                        + offs_k[None, :]
+                    )
+                    for k_offset in range(0, K, BLOCK_K):
+                        a = tl.load(a_ptrs, mask=offs_am[:, None] < m_size)
+                        b = tl.load(b_ptrs, mask=offs_bn[:, None] < n_size)
+                        accumulator += tl.dot(a, b.T)
+                        a_ptrs += BLOCK_K
+                        b_ptrs += BLOCK_K
 
                 offs_am = tile_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)
                 offs_bn = tile_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)
-                scale_a = tl.load(
-                    scale_a_ptr
-{%- if A_IS_2D %}
-                    + m_scale_start_offset
-{%- else %}
-                    + g * SCALE_A_STRIDE_G
-{%- endif %}
-                    + offs_am[:, None],
+                a_scale = tl.load(
+                    a_scale_ptr + M_start_offset + offs_am[:, None],
                     mask=offs_am[:, None] < m_size,
                 )
-                scale_b = tl.load(
-                    scale_b_ptr
-{%- if B_IS_2D %}
-                    + n_scale_start_offset
-{%- else %}
-                    + g * SCALE_B_STRIDE_G
-{%- endif %}
-                    + offs_bn[None, :],
+                b_scale = tl.load(
+                    b_scale_ptr + N_start_offset + offs_bn[None, :],
                     mask=offs_bn[None, :] < n_size,
                 )
-                c = accumulator.to(tl.float32) * scale_a * scale_b
+                c = accumulator.to(tl.float32) * a_scale * b_scale
 
-{%- if M_IS_VARYING %}
-                idx_m = (m_start_offset + offs_am[:, None])
-{%- else %}
-                idx_m = offs_am[:, None]
-{%- endif %}
-{%- if N_IS_VARYING %}
-                idx_n = (n_start_offset + offs_bn[None, :])
-{%- else %}
+                idx_m = (M_start_offset + offs_am[:, None])
                 idx_n = offs_bn[None, :]
-{%- endif %}
                 mask = offs_am[:, None] < m_size and offs_bn[None, :] < n_size
-{%- if M_IS_VARYING or N_IS_VARYING %}
                 {{store_output(("idx_m", "idx_n"), "c", "mask", indent_width=16)}}
-{%- else %}
-                {{store_output(("g", "idx_m", "idx_n"), "c", "mask", indent_width=16)}}
-{%- endif %}
                 tidx += NUM_SMS
 
             iterated_tiles += num_tiles
@@ -394,7 +286,7 @@
 triton_scaled_grouped_mm_template = TritonTemplate(
     name="scaled_grouped_mm",
     grid=persistent_grouped_mm_grid,
-    source=triton_grouped_mm_source,
+    source=triton_scaled_grouped_mm_source,
 )
 
 
@@ -405,9 +297,7 @@
     layout=None,
     out_dtype=None,
 ):
-    mat1, mat2 = realize_inputs(mat1, mat2)
-    if offs is not None:
-        realize_inputs(offs)
+    mat1, mat2, offs = realize_inputs(mat1, mat2, offs)
     mat1_size = mat1.get_size()
     mat2_size = mat2.get_size()
 
@@ -458,99 +348,87 @@
     mat_b: TensorBox,
     offs: Optional[TensorBox],
     bias: Optional[TensorBox],
-    scale_result: Optional[TensorBox],
 ) -> bool:
-    if not has_triton_tma_device():
-        return False
+    a_shape = mat_a.get_size()
+    b_shape = mat_b.get_size()
+    a_stride = mat_a.get_stride()
+    b_stride = mat_b.get_stride()
+
+    # A must be contiguous 2d
+    a_layout_ok = (
+        len(a_shape) == 2
+        and a_stride[1] == 1
+        and a_stride[0] == a_shape[1]
+        and a_shape[1] >= 32
+    )
 
-    # The _grouped_mm()/_scaled_grouped_mm() operator do not support
-    # bias nor scale_result yet.
-    if bias is not None:
-        return False
-    if scale_result is not None:
-        return False
+    # B must be contiguous 3d with transposed last dimension
+    b_layout_ok = (
+        len(b_shape) == 3
+        and b_stride[2] == b_shape[1]
+        and b_stride[1] == 1
+        and b_stride[0] == (b_shape[1] * b_shape[2])
+        and b_shape[1] >= 32
+    )
 
-    if len(mat_a.get_size()) == 2 or len(mat_b.get_size()) == 2:
-        return offs is not None
-    else:
-        return offs is None
+    return (
+        offs is not None
+        and bias is None
+        and has_triton_tma_device()
+        and a_layout_ok
+        and b_layout_ok
+    )
 
 
 def create_offsets(x, m1_size, m2_size, offs_size):
-    m1_is_2d = len(m1_size) == 2
-    m2_is_2d = len(m2_size) == 2
-    if m1_is_2d:
-        if m2_is_2d:
-            k = V.graph.sizevars.size_hint(m1_size[1])
-            noffs = V.graph.sizevars.size_hint(offs_size[0])
-            step = k / noffs
-            return torch.linspace(
-                step, k, noffs, dtype=x.get_dtype(), device=x.get_device()
-            )
-
-        else:
-            m = V.graph.sizevars.size_hint(m1_size[0])
-            noffs = V.graph.sizevars.size_hint(offs_size[0])
-            step = m / noffs
-            return torch.linspace(
-                step, m, noffs, dtype=x.get_dtype(), device=x.get_device()
-            )
-    else:
-        if m2_is_2d:
-            n = V.graph.sizevars.size_hint(m2_size[0])
-            noffs = V.graph.sizevars.size_hint(offs_size[0])
-            step = n / noffs
-            return torch.linspace(
-                step, n, noffs, dtype=x.get_dtype(), device=x.get_device()
-            )
-        else:
-            return None
+    assert len(m1_size) == 2 and len(m2_size) == 3, (
+        "Autotuning _scaled_grouped_mm is only implemented for 2d-3d tensors"
+    )
+    m = V.graph.sizevars.size_hint(m1_size[0])
+    noffs = V.graph.sizevars.size_hint(offs_size[0])
+    step = m / noffs
+    return torch.linspace(step, m, noffs, dtype=x.get_dtype(), device=x.get_device())
 
 
-def _tuned_grouped_mm_common(
-    operator_name: str,
-    algorithm_name: str,
-    extern_kernel_choice: ExternKernelChoice,
-    kernel_template: TritonTemplate,
+@register_lowering(aten._scaled_grouped_mm.default, type_promotion_kind=None)
+def tuned_scaled_grouped_mm(
     mat_a: TensorBox,
     mat_b: TensorBox,
-    scale_a: Optional[TensorBox] = None,
-    scale_b: Optional[TensorBox] = None,
+    scale_a: TensorBox,
+    scale_b: TensorBox,
     offs: Optional[TensorBox] = None,
     bias: Optional[TensorBox] = None,
     scale_result: Optional[TensorBox] = None,
     out_dtype: Optional[torch.dtype] = None,
-    use_fast_accum: Optional[bool] = False,
+    use_fast_accum: bool = False,
     layout: Optional[Layout] = None,
 ) -> TensorBox:
-    assert (scale_a is None) == (scale_b is None)
-    assert scale_result is None or scale_a is not None
-
     m1_size, m2_size, layout, mat_a, mat_b, offs = grouped_mm_args(
         mat_a, mat_b, offs, layout=layout, out_dtype=out_dtype
     )
-    counters["aten_mm_info"][operator_name] += 1
-    log_message = f"Tuned {operator_name}: mat1_shape=%s, mat2_shape=%s, mat1_dtype=%s, mat2_dtype=%s, output_layout=%s"
+
+    counters["aten_mm_info"]["aten._scaled_grouped_mm.default"] += 1
     log.info(
-        log_message,
+        "Tuned aten._scaled_grouped_mm.default: mat1_shape=%s, mat2_shape=%s, mat1_dtype=%s, mat2_dtype=%s, output_layout=%s",
         m1_size,
         m2_size,
         mat_a.get_dtype(),
         mat_b.get_dtype(),
         layout,
     )
+
     check_supported_striding(mat_a, mat_b)
 
+    scale_a, scale_b = realize_inputs(scale_a, scale_b)
+
     # workaround for Inductor not supporting optional tensor input arguments
-    input_nodes: list[Any] = [mat_a, mat_b]
-    if scale_a is not None:
-        input_nodes.append(realize_inputs(scale_a))
-    if scale_b is not None:
-        input_nodes.append(realize_inputs(scale_b))
+    input_nodes: list[Any] = [mat_a, mat_b, scale_a, scale_b]
     if offs is not None:
         input_nodes.append(realize_inputs(offs))
+    if bias is not None:
+        input_nodes.append(realize_inputs(bias))
 
-    aten_choice = extern_kernel_choice.bind(
+    aten_choice = aten__scaled_grouped_mm.bind(
         input_nodes,
         layout,
         out_dtype=out_dtype,
@@ -563,52 +441,30 @@
 
     _, is_nonzero = _is_static_problem(layout)
 
-    # Checking only for the equality of correspoding dims of
-    # multiplicands here, relying on meta function checks for
-    # everything else.
-    if is_nonzero and can_use_triton_kernel(mat_a, mat_b, offs, bias, scale_result):
-        if len(m1_size) == 2:
-            if len(m2_size) == 2:
-                m, k1 = m1_size
-                k2, _ = m2_size
-                g = offs.get_size()[0]
-                V.graph.sizevars.guard_equals(k1, k2)
-                a_is_2d, b_is_2d = True, True
-            else:
-                g1 = offs.layout.size[0]
-                m, k1 = m1_size
-                g2, k2, _ = m2_size
-                g = V.graph.sizevars.guard_equals(g1, g2)
-                V.graph.sizevars.guard_equals(k1, k2)
-                a_is_2d, b_is_2d = True, False
-        else:
-            if len(m2_size) == 2:
-                g1 = offs.layout.size[0]
-                g2, m, k1 = m1_size
-                k2, _ = m2_size
-                g = V.graph.sizevars.guard_equals(g1, g2)
-                V.graph.sizevars.guard_equals(k1, k2)
-                a_is_2d, b_is_2d = False, True
-            else:
-                g1, m, k1 = m1_size
-                g2, k2, _ = m2_size
-                g = V.graph.sizevars.guard_equals(g1, g2)
-                V.graph.sizevars.guard_equals(k1, k2)
-                a_is_2d, b_is_2d = False, False
-
+    if is_nonzero and can_use_triton_kernel(mat_a, mat_b, offs, bias):
+        m, k1 = m1_size
+        g, k2, n = m2_size
+        k = V.graph.sizevars.guard_equals(k1, k2)
         kwargs = {
-            "A_IS_2D": a_is_2d,
-            "B_IS_2D": b_is_2d,
-            "USE_FAST_ACCUM": use_fast_accum,
+            "G": g,
+            "M": m,
+            "M_BUCKET": next_power_of_2(m),
+            "N": n,
+            "K": k,
             "NUM_SMS": get_num_sms(),
             "USE_TMA_LOAD": True,
+            "USE_TMA_STORE": False,
+            "USE_FAST_ACCUM": use_fast_accum,
         }
-
-        for config in early_config_prune(g, m, grouped_mm_configs(), kwargs):
-            kernel_template.maybe_append_choice(
+        for config in early_config_prune(scaled_grouped_mm_configs(), kwargs):
+            triton_scaled_grouped_mm_template.maybe_append_choice(
                 choices,
                 input_nodes=input_nodes,
                 layout=layout,
+                workspace_arg=get_tma_workspace_arg(
+                    num_tma_descriptors=2,
+                    device=mat_a.get_device(),
+                ),
                 num_stages=config.num_stages,
                 num_warps=config.num_warps,
                 **kwargs,
@@ -616,43 +472,8 @@
             )
 
     input_gen_fns = {
-        4: lambda x: create_offsets(
-            x, m1_size, m2_size, offs.get_size() if offs is not None else None
-        ),
+        4: lambda x: create_offsets(x, m1_size, m2_size, offs.get_size()),
     }
     return autotune_select_algorithm(
-        algorithm_name, choices, input_nodes, layout, input_gen_fns=input_gen_fns
-    )
-
-
-@register_lowering(aten._scaled_grouped_mm.default, type_promotion_kind=None)
-def tuned_scaled_grouped_mm(
-    mat_a: TensorBox,
-    mat_b: TensorBox,
-    scale_a: TensorBox,
-    scale_b: TensorBox,
-    offs: Optional[TensorBox] = None,
-    bias: Optional[TensorBox] = None,
-    scale_result: Optional[TensorBox] = None,
-    out_dtype: Optional[torch.dtype] = None,
-    use_fast_accum: bool = False,
-    layout: Optional[Layout] = None,
-) -> TensorBox:
-    """Auto-tuning for _scaled_grouped_mm() operator."""
-
-    return _tuned_grouped_mm_common(
-        "aten._scaled_grouped_mm.default",
-        "scaled_grouped_mm",
-        aten__scaled_grouped_mm,
-        triton_scaled_grouped_mm_template,
-        mat_a,
-        mat_b,
-        scale_a,
-        scale_b,
-        offs,
-        bias,
-        scale_result,
-        out_dtype,
-        use_fast_accum,
-        layout,
+        "scaled_grouped_mm", choices, input_nodes, layout, input_gen_fns=input_gen_fns
     )
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/mkldnn_ir.py torch/_inductor/mkldnn_ir.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/mkldnn_ir.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/mkldnn_ir.py	2025-08-05 12:18:44.660448500 +1000
@@ -690,7 +690,7 @@
         if isinstance(self.layout, Layout):
             self.codegen_size_asserts(wrapper)
 
-    def get_mutation_names(self):
+    def get_mutation_names(self) -> Sequence[str]:
         return [self.inputs[self.idx_for_inplace_sum].get_name()]
 
     def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]:
@@ -1045,7 +1045,7 @@
         if isinstance(self.layout, Layout):
             self.codegen_size_asserts(wrapper)
 
-    def get_mutation_names(self):
+    def get_mutation_names(self) -> Sequence[str]:
         binary_post_op = self.constant_args[-5]
         if binary_post_op == "sum":
             return [self.inputs[self.idx_for_inplace_sum].get_name()]
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/runtime/triton_heuristics.py torch/_inductor/runtime/triton_heuristics.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/runtime/triton_heuristics.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/runtime/triton_heuristics.py	2025-08-05 12:18:44.729836200 +1000
@@ -33,7 +33,6 @@
 from torch._dynamo.utils import set_feature_use
 from torch._prims_common import compute_required_storage_length
 from torch.utils._ordered_set import OrderedSet
-from torch.utils._triton import triton_set_allocator
 
 from ..triton_bundler import TritonBundler
 from ..utils import prefix_is_reduction, triton_version_uses_attrs_dict
@@ -1115,8 +1114,6 @@
                 **self.configs[0].kwargs,
             )
 
-        triton_set_allocator(self.triton_meta["device"])
-
         if len(self.launchers) != 1:
             if len(self.launchers) == 0:
                 start_time = time.time_ns()
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/utils.py torch/_inductor/utils.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_inductor/utils.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_inductor/utils.py	2025-08-05 23:42:41.286435300 +1000
@@ -994,6 +994,25 @@
     return last_node
 
 
+def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]:
+    placeholder_nodes = gm.graph.find_nodes(op="placeholder")
+    input_devices: OrderedSet[torch.device] = OrderedSet(
+        node.meta["val"].device
+        for node in placeholder_nodes
+        if isinstance(node.meta.get("val"), torch.Tensor)
+    )
+
+    out_arg = output_node(gm).args[0]  # type: ignore[union-attr]
+    out_args = out_arg if isinstance(out_arg, tuple) else (out_arg,)
+    out_devices: OrderedSet[torch.device] = OrderedSet(
+        arg.meta["val"].device
+        for arg in out_args
+        if isinstance(arg, torch.fx.Node)
+        and isinstance(arg.meta.get("val"), torch.Tensor)
+    )
+    return input_devices | out_devices
+
+
 _registered_caches: list[Any] = []
 
 
@@ -1402,6 +1421,17 @@
             return False
         return True
 
+
+    arch = os.environ.get("TRITON_OVERRIDE_ARCH")
+    if arch is None:
+        print("\033[91m%%% [warn] No TRITON_OVERRIDE_ARCH set - no max_autotune_gemm for you until you set it.\033[0m")
+        return False
+
+    arch = arch.lower()
+    if arch.startswith("gfx11") or arch.startswith("gfx12"):
+        return True
+
+
     min_sms = 16 if device.type == "xpu" else 68  # 3080
     avail_sms = prop.multi_processor_count
     if avail_sms < min_sms:
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_meta_registrations.py torch/_meta_registrations.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_meta_registrations.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_meta_registrations.py	2025-08-05 12:18:44.519195800 +1000
@@ -7292,7 +7292,7 @@
     return torch.empty_like(self, dtype=result_dtype)
 
 
-def _compute_grouped_mm_output_size(mat1, mat2, offs):
+def _compute_grouped_gemm_output_size(mat1, mat2, offs):
     mat1_is_2d = mat1.dim() == 2
     mat2_is_2d = mat2.dim() == 2
 
@@ -7316,229 +7316,158 @@
             return mat1.size(0), mat1.size(1), mat2.size(-1)
 
 
-def _meta_grouped_mm_common(
-    mat_a: Tensor,
-    mat_b: Tensor,
-    scale_a: Optional[torch.Tensor],
-    scale_b: Optional[torch.Tensor],
+@register_meta(aten._grouped_mm)
+@out_wrapper()
+def grouped_mm(
+    mat1: Tensor,
+    mat2: Tensor,
     offs: Optional[Tensor] = None,
     bias: Optional[Tensor] = None,
+    out_dtype: Optional[torch.dtype] = None,
+) -> Tensor:
+    torch._check(mat1.dim() == 2 or mat1.dim() == 3, lambda: "mat1 must be 2d or 3d")
+    torch._check(mat2.dim() == 2 or mat2.dim() == 3, lambda: "mat2 must be 2d or 3d")
+    torch._check(
+        (offs is not None) == (mat1.dim() == 2 or mat2.dim() == 2),
+        lambda: "Have to provide offsets if there is a 2d matrix, or no offset if both matrices are 3d",
+    )
+
+    if offs is not None:
+        torch._check(offs.dim() == 1, lambda: "offsets must be 1d")
+
+    out_dtype = out_dtype or mat1.dtype
+    torch._check(bias is None, lambda: "bias not supported yet")
+
+    out_size = _compute_grouped_gemm_output_size(mat1, mat2, offs)
+    out = mat1.new_empty(out_size, dtype=out_dtype)
+
+    return out
+
+
+@register_meta([aten._scaled_grouped_mm.default])
+def meta_scaled_grouped_mm(
+    mat_a: torch.Tensor,
+    mat_b: torch.Tensor,
+    scale_a: torch.Tensor,
+    scale_b: torch.Tensor,
+    offs: Optional[torch.Tensor] = None,
+    bias: Optional[torch.Tensor] = None,
     scale_result: Optional[torch.Tensor] = None,
     out_dtype: Optional[torch.dtype] = None,
     use_fast_accum: bool = False,
 ):
+    # Check dimensions
     torch._check(
-        (scale_a is None) == (scale_b is None),
-        lambda: "Either both scale factors are given, or none",
+        mat_a.dim() == 2 or mat_a.dim() == 3, lambda: "mat_a has to be 2 or 3d"
+    )
+    torch._check(
+        mat_b.dim() == 2 or mat_b.dim() == 3, lambda: "mat_b has to be 2 or 3d"
     )
-    scaled = scale_a is not None and scale_b is not None
-
-    # Implementing all the checks from
-    # _grouped_mm_cuda()/_scaled_grouped_mm_cuda() code in
-    # aten/src/ATen/native/cuda/Blas.cpp.
 
-    if scaled:
-        torch._check(
-            mat_a.dtype == torch.float8_e4m3fn and mat_b.dtype == torch.float8_e4m3fn,
-            lambda: f"Expected inputs of E4M3 FP8 type but got mat_a.dtype={mat_a.dtype} and mat_b.dtype={mat_b.dtype}.",
-        )
-    else:
-        torch._check(
-            mat_a.dtype == torch.bfloat16 and mat_b.dtype == torch.bfloat16,
-            lambda: f"Expected inputs of BF16 type but got mat_a.dtype={mat_a.dtype} and mat_b.dtype={mat_b.dtype}.",
-        )
+    a_is_2d = mat_a.dim() == 2
+    b_is_2d = mat_b.dim() == 2
 
+    # Check offsets
     torch._check(
-        mat_a.dim() in [2, 3] and mat_b.dim() in [2, 3],
-        lambda: f"Multiplicands must be 2D or 3D but got mat_a.dim()={mat_a.dim()} and mat_b.dim()={mat_b.dim()}",
+        (offs is not None) == (a_is_2d or b_is_2d),
+        lambda: "Have to provide offsets if there is a 2d matrix",
     )
 
-    mat_a_is_2d = mat_a.dim() == 2
-    mat_b_is_2d = mat_b.dim() == 2
+    if offs is not None:
+        torch._check(offs.dim() == 1, lambda: "offs has to be 1D")
+        torch._check(offs.dtype == torch.int, lambda: "Offsets have to be int32")
 
+    # Check matrix sizes
     torch._check(
-        mat_a.shape[-1] % 16 == 0,
-        lambda: f"Expected mat_a.shape[-1] to be divisible by 16, but got mat_a.shape[-1]={mat_a.shape[1]}",
+        mat_a.size(-1) % 16 == 0,
+        lambda: f"Expected trailing dimension of mat_a to be divisible by 16 but got mat1 shape: {mat_a.size()}",
     )
     torch._check(
-        mat_b.shape[-2] % 16 == 0 and mat_b.shape[-1] % 16 == 0,
-        lambda: f"Expected mat_b.shape[-2] and mat_b.shape[-1] to be both divisble by 16 but got {mat_b.shape[-2]} and {mat_b.shape[-1]}",  # noqa: B950
+        mat_b.size(-2) % 16 == 0 and mat_b.size(-1) % 16 == 0,
+        lambda: f"Expected mat_b shape to be divisible by 16 but got mat_b shape: {mat_b.size()}",
     )
 
-    if scaled:
-
-        def is_row_major(mat):
-            mat_stride = mat.stride()
-            return mat_stride[-2] > 1 and mat_stride[-1] == 1
+    # Check scales
+    torch._check(
+        scale_a.dtype == torch.float and scale_b.dtype == torch.float,
+        lambda: "Both scale_a and scale_b must be float (fp32) tensors.",
+    )
 
-        def is_col_major(mat):
-            mat_stride = mat.stride()
-            return mat_stride[-2] == 1 and mat_stride[-1] > 1
+    # Check scale dimensions
+    scale_multiplier = offs.size(0) if (a_is_2d and b_is_2d) else 1  # type: ignore[union-attr]
 
+    if a_is_2d:
         torch._check(
-            is_row_major(mat_a),
-            lambda: f"Expected mat_a tensor to be row major in the last two dimensions, got strides {mat_a.stride()[-2:]}",
+            scale_a.dim() == 1,
+            lambda: f"scale must be a 1D tensor for 2D mat_a, but got {scale_a.dim()}D",
         )
+        torch._check(scale_a.is_contiguous(), lambda: "scale_a must be contiguous")
         torch._check(
-            is_col_major(mat_b),
-            lambda: f"Expected mat_b tensor to be column major in the last two dimensions, got strides {mat_b.stride()[-2:]}",
+            scale_a.size(0) == mat_a.size(0) * scale_multiplier,
+            lambda: "scale must have the same length as mat_a",
         )
-
-    def check_valid_strides(mat_name, mat):
-        end_dim = mat.dim() - 1
-        alignment = 16 / mat.element_size()
-        mat_stride = mat.stride()
-        if mat_stride[end_dim - 1] == 1 and mat_stride[end_dim] >= max(
-            1, mat.shape[end_dim - 1]
-        ):
-            torch._check(
-                mat_stride[end_dim] % alignment == 0,
-                lambda: f"Expected {mat_name} stride along {end_dim} dim to be multiple of 16 bytes, got {mat_stride[end_dim]}.",
-            )
-        elif mat_stride[end_dim] == 1 and mat_stride[end_dim - 1] >= max(
-            1, mat.shape[end_dim]
-        ):
-            torch._check(
-                mat_stride[end_dim - 1] % alignment == 0,
-                lambda: f"Expected {mat_name} stride along {end_dim - 1} dim to be multiple of 16 bytes, got {mat_stride[end_dim - 1]}.",  # noqa: B950
-            )
-        else:
-            torch._check(
-                False,
-                lambda: f"Expected {mat_name} to have a contiguous dimension and not be mat_a-overlapping, got {mat_stride} for strides and {mat.shape} for sizes.",  # noqa: B950
-            )
-
-    check_valid_strides("mat_a", mat_a)
-    check_valid_strides("mat_b", mat_b)
-
-    if scale_a is not None and scale_b is not None:
+    else:
         torch._check(
-            scale_a.dtype == torch.float32 and scale_b.dtype == torch.float32,
-            lambda: "Both scale_a and scale_b must be float (fp32) tensors, but got scale_a.dtype={scale_a.dtype} and scale_b.dtype={scale_b.dtype}.",  # noqa: B950
+            scale_a.dim() == 2,
+            lambda: f"scale must be a 2D tensor for 3D mat_a, but got {scale_a.dim()}D",
         )
-
-        def check_scale(scale_name, scale, mat, scaled_dim, scale_multiplier=1):
-            if mat.dim() == 2:
-                torch._check(
-                    scale.dim() == 1,
-                    lambda: f"Expected {scale_name} to be 1D tensor, but got {scale.dim()}D tensor.",
-                )
-                torch._check(
-                    scale.is_contiguous(),
-                    lambda: f"Expected {scale_name} to be contiguous.",
-                )
-                torch._check(
-                    scale.shape[0] == mat.shape[scaled_dim] * scale_multiplier,
-                    lambda: f"Expected {scale_name} to have {mat.shape[scaled_dim] * scale_multiplier} elements, got {scale.shape[0]} elements.",  # noqa: B950
-                )
-            else:
-                torch._check(
-                    scale.dim() == 2,
-                    lambda: f"Expected {scale_name} to be 2D tensor, but got {scale.dim()}D tensor.",
-                )
-                torch._check(
-                    scale.stride(1) == 1,
-                    lambda: f"Expected {scale_name} to be contiguous in the last dimension.",
-                )
-                torch._check(
-                    scale.shape[0] == mat.shape[0],
-                    lambda: f"Expected {scale_name} batch dimension to be {mat.shape[0]}, got {scale.shape[0]}.",
-                )
-                torch._check(
-                    scale.shape[1] == mat.shape[1 + scaled_dim],
-                    lambda: f"Expected {scale_name} non-batch dimension to be {mat.shape[1 + scaled_dim]}, got {scale.shape[1]}.",
-                )
-
-        scale_multiplier = (
-            offs.shape[0] if offs is not None and mat_a_is_2d and mat_b_is_2d else 1
-        )
-        check_scale("scale_a", scale_a, mat_a, 0, scale_multiplier)
-        check_scale("scale_b", scale_b, mat_b, 1, scale_multiplier)
-
-        torch._check(
-            scale_result is None,
-            lambda: "Scale result tensor provided, but it is not supported yet.",
+        torch._check(
+            scale_a.stride(1) == 1,
+            lambda: "scale_a must be contiguous in the last dimension",
+        )
+        torch._check(
+            scale_a.size(0) == mat_a.size(0),
+            lambda: "scale must have the same batch dimension as mat_a",
+        )
+        torch._check(
+            scale_a.size(1) == mat_a.size(1),
+            lambda: "scale must have the same first dimension as mat_a",
         )
 
-    if mat_a_is_2d or mat_b_is_2d:
+    # Similar checks for scale_b
+    if b_is_2d:
         torch._check(
-            offs is not None,
-            lambda: f"Offsets tensor not provided, but is needed for {mat_a.dim()}D/{mat_b.dim()}D multiplicand layouts.",
+            scale_b.dim() == 1,
+            lambda: f"scale must be a 1D tensor for 2D mat_b, but got {scale_b.dim()}D",
+        )
+        torch._check(scale_b.is_contiguous(), lambda: "scale_b must be contiguous")
+        torch._check(
+            scale_b.size(0) == mat_b.size(1) * scale_multiplier,
+            lambda: "scale must have the same length as mat_b",
         )
-        if offs is not None:  # to silence Mypy
-            torch._check(
-                offs.dim() == 1,
-                lambda: f"Offsets tensor must be 1D, but got offs.dim()={offs.dim()}.",
-            )
-            torch._check(
-                offs.dtype == torch.int32,
-                lambda: f"Offsets tensor must be integer (int32) tensor, but got {offs.dtype}.",
-            )
     else:
         torch._check(
-            offs is None,
-            lambda: "Offsets tensor provided, but is not needed for 3D/3D multiplicand layouts.",
+            scale_b.dim() == 2,
+            lambda: f"scale must be a 2D tensor for 3D mat_b, but got {scale_b.dim()}D",
+        )
+        torch._check(
+            scale_b.stride(1) == 1,
+            lambda: "scale_b must be contiguous in the last dimension",
+        )
+        torch._check(
+            scale_b.size(0) == mat_b.size(0),
+            lambda: "scale must have the same batch dimension as mat_b",
+        )
+        torch._check(
+            scale_b.size(1) == mat_b.size(2),
+            lambda: "scale must have the same last dimension as mat_b",
         )
 
-    torch._check(
-        bias is None,
-        lambda: "Bias tensor provided, but it is not supported yet.",
-    )
+    # Check bias
+    torch._check(bias is None, lambda: "Bias not supported yet")
 
+    # Check output dtype
+    out_dtype_ = out_dtype if out_dtype is not None else mat_a.dtype
     torch._check(
-        out_dtype is None or out_dtype == torch.bfloat16,
-        lambda: "If output dtype provided, it must be torch.bfloat16.",
+        out_dtype_ == torch.bfloat16,
+        lambda: "Only bf16 high precision output types are supported for grouped gemm",
     )
 
-    out_size = _compute_grouped_mm_output_size(mat_a, mat_b, offs)
-    out_dtype = out_dtype or mat_a.dtype
-    return torch.empty(out_size, dtype=out_dtype, device=mat_a.device)
-
+    # Compute output size
+    out_size = _compute_grouped_gemm_output_size(mat_a, mat_b, offs)
+    out = mat_a.new_empty(out_size, dtype=out_dtype)
 
-@register_meta(aten._grouped_mm)
-@out_wrapper()
-def grouped_mm(
-    mat_a: Tensor,
-    mat_b: Tensor,
-    offs: Optional[Tensor] = None,
-    bias: Optional[Tensor] = None,
-    out_dtype: Optional[torch.dtype] = None,
-) -> Tensor:
-    return _meta_grouped_mm_common(
-        mat_a,
-        mat_b,
-        scale_a=None,
-        scale_b=None,
-        offs=offs,
-        bias=bias,
-        scale_result=None,
-        out_dtype=out_dtype,
-    )
-
-
-@register_meta([aten._scaled_grouped_mm.default])
-def meta_scaled_grouped_mm(
-    mat_a: torch.Tensor,
-    mat_b: torch.Tensor,
-    scale_a: torch.Tensor,
-    scale_b: torch.Tensor,
-    offs: Optional[torch.Tensor] = None,
-    bias: Optional[torch.Tensor] = None,
-    scale_result: Optional[torch.Tensor] = None,
-    out_dtype: Optional[torch.dtype] = None,
-    use_fast_accum: bool = False,
-):
-    return _meta_grouped_mm_common(
-        mat_a,
-        mat_b,
-        scale_a=scale_a,
-        scale_b=scale_b,
-        offs=offs,
-        bias=bias,
-        scale_result=scale_result,
-        out_dtype=out_dtype,
-        use_fast_accum=use_fast_accum,
-    )
+    return out
 
 
 @register_meta(aten._softmax)
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/_subclasses/fake_tensor.py torch/_subclasses/fake_tensor.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/_subclasses/fake_tensor.py	2025-06-10 07:37:18.000000000 +1000
+++ torch/_subclasses/fake_tensor.py	2025-08-05 12:18:44.758625600 +1000
@@ -122,11 +122,6 @@
 
 
 @dataclass
-class UnsupportedMutationAliasingException(RuntimeError):
-    reason: str
-
-
-@dataclass
 class MetadataMismatchError(RuntimeError):
     reason: str
 
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/elastic/agent/server/api.py torch/distributed/elastic/agent/server/api.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/elastic/agent/server/api.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/distributed/elastic/agent/server/api.py	2025-08-05 12:18:44.928017900 +1000
@@ -71,8 +71,7 @@
         tee: tees the specified std stream(s) to console + file,
              selectively tee for a particular local rank by passing a map,
              takes precedence over ``redirects`` settings.
-        event_log_handler: name of the event logging handler as registered in
-          `elastic/events/handlers.py <https://docs.pytorch.org/docs/stable/elastic/events.html>`_.
+
     """
 
     role: str
@@ -87,7 +86,6 @@
     master_port: Optional[int] = None
     master_addr: Optional[str] = None
     local_addr: Optional[str] = None
-    event_log_handler: str = "null"
 
     def __post_init__(self):
         assert self.local_world_size > 0
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/launcher/api.py torch/distributed/launcher/api.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/launcher/api.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/distributed/launcher/api.py	2025-08-05 12:18:44.959270000 +1000
@@ -64,9 +64,6 @@
         local_addr: address of the local node if any. If not set, a lookup on the local
                 machine's FQDN will be performed.
         local_ranks_filter: ranks for which to show logs in console. If not set, show from all.
-        event_log_handler: name of the event logging handler as registered in
-          `elastic/events/handlers.py <https://docs.pytorch.org/docs/stable/elastic/events.html>`_.
-
 
     .. note::
         `rdzv_timeout` is a legacy argument that will be removed in future.
@@ -90,7 +87,6 @@
     log_line_prefix_template: Optional[str] = None
     metrics_cfg: dict[str, str] = field(default_factory=dict)
     local_addr: Optional[str] = None
-    event_log_handler: str = "null"
 
     def __post_init__(self):
         default_timeout = 900
@@ -198,19 +194,18 @@
 
     logger.info(
         "Starting elastic_operator with launch configs:\n"
-        "  entrypoint         : %(entrypoint)s\n"
-        "  min_nodes          : %(min_nodes)s\n"
-        "  max_nodes          : %(max_nodes)s\n"
-        "  nproc_per_node     : %(nproc_per_node)s\n"
-        "  run_id             : %(run_id)s\n"
-        "  rdzv_backend       : %(rdzv_backend)s\n"
-        "  rdzv_endpoint      : %(rdzv_endpoint)s\n"
-        "  rdzv_configs       : %(rdzv_configs)s\n"
-        "  max_restarts       : %(max_restarts)s\n"
-        "  monitor_interval   : %(monitor_interval)s\n"
-        "  log_dir            : %(log_dir)s\n"
-        "  metrics_cfg        : %(metrics_cfg)s\n"
-        "  event_log_handler  : %(event_log_handler)s\n",
+        "  entrypoint       : %(entrypoint)s\n"
+        "  min_nodes        : %(min_nodes)s\n"
+        "  max_nodes        : %(max_nodes)s\n"
+        "  nproc_per_node   : %(nproc_per_node)s\n"
+        "  run_id           : %(run_id)s\n"
+        "  rdzv_backend     : %(rdzv_backend)s\n"
+        "  rdzv_endpoint    : %(rdzv_endpoint)s\n"
+        "  rdzv_configs     : %(rdzv_configs)s\n"
+        "  max_restarts     : %(max_restarts)s\n"
+        "  monitor_interval : %(monitor_interval)s\n"
+        "  log_dir          : %(log_dir)s\n"
+        "  metrics_cfg      : %(metrics_cfg)s\n",
         {
             "entrypoint": entrypoint_name,
             "min_nodes": config.min_nodes,
@@ -224,7 +219,6 @@
             "monitor_interval": config.monitor_interval,
             "log_dir": config.logs_specs.root_log_dir,  # type: ignore[union-attr]
             "metrics_cfg": config.metrics_cfg,
-            "event_log_handler": config.event_log_handler,
         },
     )
 
@@ -251,7 +245,6 @@
         master_addr=master_addr,
         master_port=master_port,
         local_addr=config.local_addr,
-        event_log_handler=config.event_log_handler,
     )
 
     agent = LocalElasticAgent(
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/run.py torch/distributed/run.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/distributed/run.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/distributed/run.py	2025-08-05 12:18:44.881142400 +1000
@@ -487,14 +487,6 @@
         help="Multiprocessing start method to use when creating workers.",
     )
     parser.add_argument(
-        "--event-log-handler",
-        "--event_log_handler",
-        action=env,
-        type=str,
-        default="null",
-        help="name of a registered event logging handler (see: https://docs.pytorch.org/docs/stable/elastic/events.html)",
-    )
-    parser.add_argument(
         "--role",
         action=env,
         type=str,
@@ -825,7 +817,6 @@
         log_line_prefix_template=log_line_prefix_template,
         local_addr=args.local_addr,
         logs_specs=logs_specs,
-        event_log_handler=args.event_log_handler,
     )
 
     with_python = not args.no_python
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/include/caffe2/core/macros.h torch/include/caffe2/core/macros.h
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/include/caffe2/core/macros.h	2025-06-10 07:45:22.000000000 +1000
+++ torch/include/caffe2/core/macros.h	2025-08-05 12:18:47.757933100 +1000
@@ -42,7 +42,7 @@
   {"CUDA_VERSION", "11.8"}, \
   {"ROCM_VERSION", ""}, \
   {"USE_CUDNN", "ON"}, \
-  {"COMMIT_SHA", "0f61f5f414165900730fd86cf58b6e1aae6f5b52"}, \
+  {"COMMIT_SHA", "843156205ec57cf78d5cf10bf982656b3db51dfd"}, \
   {"CUDNN_VERSION", "9.1.0"}, \
   {"USE_NCCL", "OFF"}, \
   {"USE_MPI", "OFF"}, \
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/onnx/_internal/exporter/_core.py torch/onnx/_internal/exporter/_core.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/onnx/_internal/exporter/_core.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/onnx/_internal/exporter/_core.py	2025-08-05 12:19:13.296644400 +1000
@@ -619,17 +619,10 @@
         node_name_to_values[node.name] = outputs
         for i, output in enumerate(outputs):
             output.name = f"{node.name}__{i}"
-            # Set the name of the producing node using the value name for correspondence
-            producer = output.producer()
-            if producer is not None:
-                producer.name = f"node_{output.name}"
     else:
         _set_shape_type(outputs, node.meta["val"], complex_to_float=True)
         node_name_to_values[node.name] = outputs
         outputs.name = node.name
-        producer = outputs.producer()
-        if producer is not None:
-            producer.name = f"node_{outputs.name}"
 
     for ir_node in onnx_nodes:
         ir_node.meta["node"] = node
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/profiler/profiler.py torch/profiler/profiler.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/profiler/profiler.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/profiler/profiler.py	2025-08-05 12:19:13.358850600 +1000
@@ -157,7 +157,6 @@
         self.acc_events = acc_events
         self.custom_trace_id_callback = custom_trace_id_callback
         self.profiler: Optional[prof.profile] = None
-        self.has_cudagraphs = False
         self.mem_tl: Optional[MemoryProfileTimeline] = None
         self.use_device = None
         if ProfilerActivity.CUDA in self.activities:
@@ -182,10 +181,6 @@
         self.stop_trace()
 
     def prepare_trace(self):
-        if hasattr(torch, "_inductor"):
-            import torch._inductor.config as inductor_config
-
-            self.has_cudagraphs = inductor_config.triton.cudagraphs
         if (self.profiler is None) or (not self.acc_events):
             self.profiler = prof.profile(
                 use_cpu=(ProfilerActivity.CPU in self.activities),
@@ -226,23 +221,26 @@
                     "distributedInfo", json.dumps(dist_info, cls=_NumpyEncoder)
                 )
 
-            cuda_version = None
-            if hasattr(torch, "version"):
-                from torch.torch_version import TorchVersion
-
-                cuda_version = TorchVersion(getattr(torch.version, "cuda", "0.0"))
-
-            if self.has_cudagraphs and (
-                (cuda_version and cuda_version < "12.6")
-                or not profiler_allow_cudagraph_cupti_lazy_reinit_cuda12()
-            ):
-                os.environ["DISABLE_CUPTI_LAZY_REINIT"] = "1"
-                self.add_metadata_json("DISABLE_CUPTI_LAZY_REINIT", "1")
-                # FIXME: CUDA Graph does not work well with CUPTI teardown.
-                #   1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11)
-                #   2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12)
-                # Workaround: turn off CUPTI teardown when using CUDA Graphs.
-                os.environ["TEARDOWN_CUPTI"] = "0"
+            if hasattr(torch, "_inductor"):
+                import torch._inductor.config as inductor_config
+
+                cuda_version = None
+                if hasattr(torch, "version"):
+                    from torch.torch_version import TorchVersion
+
+                    cuda_version = TorchVersion(getattr(torch.version, "cuda", "0.0"))
+
+                if inductor_config.triton.cudagraphs and (
+                    (cuda_version and cuda_version < "12.6")
+                    or not profiler_allow_cudagraph_cupti_lazy_reinit_cuda12()
+                ):
+                    os.environ["DISABLE_CUPTI_LAZY_REINIT"] = "1"
+                    self.add_metadata_json("DISABLE_CUPTI_LAZY_REINIT", "1")
+                    # FIXME: CUDA Graph does not work well with CUPTI teardown.
+                    #   1) crashes on 1st lazy CUPTI re-init after teardown (CUDA 11)
+                    #   2) crashes on 2nd non-lazy CUPTI re-init after teardown (CUDA 12)
+                    # Workaround: turn off CUPTI teardown when using CUDA Graphs.
+                    os.environ["TEARDOWN_CUPTI"] = "0"
 
             # Insert the preset user metadata to the trace
             for k, v in self.preset_metadata.items():
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/testing/_internal/inductor_utils.py torch/testing/_internal/inductor_utils.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/testing/_internal/inductor_utils.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/testing/_internal/inductor_utils.py	2025-08-05 12:19:13.425925800 +1000
@@ -14,6 +14,15 @@
 from torch._inductor.graph import GraphLowering
 from torch._inductor.compile_fx import shape_env_from_inputs
 from torch._inductor.codecache import CppCodeCache
+from torch._inductor.custom_graph_pass import CustomGraphModulePass
+from torch._inductor.codegen.common import (
+    get_custom_backend_pass_for_device,
+    get_scheduling_for_device,
+    get_wrapper_codegen_for_device,
+    init_backend_registration,
+    register_backend_for_device
+)
+from torch._inductor.codegen.wrapper import PythonWrapperCodegen
 from torch._inductor.utils import get_gpu_shared_memory, is_big_gpu
 from torch._inductor.utils import GPU_TYPES, get_gpu_type, is_gpu
 from torch.utils._triton import has_triton
@@ -290,3 +299,41 @@
     x_fp8 = _to_fp8_saturated(x * scale, float8_dtype)
     inverse_scale = scale.reciprocal()
     return x_fp8, inverse_scale
+
+@contextlib.contextmanager
+def patch_inductor_backend(
+    device: str,
+    python_wrapper_codegen: PythonWrapperCodegen = None,
+    custom_pass: CustomGraphModulePass = None
+):
+    """
+    Patch the inductor backend for a specific device.
+    """
+    # Make sure the backend is already registered
+    init_backend_registration()
+
+    # Get the original registration parameters
+    original_scheduling = get_scheduling_for_device(device)
+    original_python_wrapper = get_wrapper_codegen_for_device(device, False)
+    original_cpp_wrapper = get_wrapper_codegen_for_device(device, True)
+    original_custom_pass = get_custom_backend_pass_for_device(device)
+
+    try:
+        # Register modified backend for the device
+        register_backend_for_device(
+            device,
+            original_scheduling,
+            python_wrapper_codegen if python_wrapper_codegen is not None else original_python_wrapper,
+            original_cpp_wrapper,
+            custom_pass if custom_pass is not None else original_custom_pass
+        )
+        yield
+    finally:
+        # Restore the original backend
+        register_backend_for_device(
+            device,
+            original_scheduling,
+            original_python_wrapper,
+            original_cpp_wrapper,
+            original_custom_pass
+        )
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/_pytree.py torch/utils/_pytree.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/_pytree.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/utils/_pytree.py	2025-08-05 12:19:13.474846000 +1000
@@ -113,14 +113,10 @@
 
 
 class EnumEncoder(json.JSONEncoder):
-    def default(self, obj: object) -> Union[str, dict[str, Any]]:
+    def default(self, obj: object) -> str:
         if isinstance(obj, Enum):
-            return {
-                "__enum__": True,
-                "fqn": f"{obj.__class__.__module__}:{obj.__class__.__qualname__}",
-                "name": obj.name,
-            }
-        return cast(str, super().default(obj))
+            return obj.value  # type: ignore[no-any-return]
+        return super().default(obj)  # type: ignore[no-any-return]
 
 
 Context = Any
@@ -1840,18 +1836,6 @@
     return _TreeSpecSchema(serialized_type_name, serialized_context, child_schemas)
 
 
-def enum_object_hook(obj: dict[str, Any]) -> Union[Enum, dict[str, Any]]:
-    if "__enum__" in obj:
-        modname, _, classname = obj["fqn"].partition(":")
-        mod = importlib.import_module(modname)
-        enum_cls = mod
-        for attr in classname.split("."):
-            enum_cls = getattr(enum_cls, attr)
-        enum_cls = cast(type[Enum], enum_cls)
-        return enum_cls[obj["name"]]
-    return obj
-
-
 def _json_to_treespec(json_schema: DumpableContext) -> TreeSpec:
     if (
         json_schema["type"] is None
@@ -1870,7 +1854,7 @@
 
     if serialize_node_def.from_dumpable_context is None:
         try:
-            context = json.loads(json_schema["context"], object_hook=enum_object_hook)
+            context = json.loads(json_schema["context"])
         except TypeError as ex:
             raise TypeError(
                 "Unable to deserialize context. "
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/_triton.py torch/utils/_triton.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/_triton.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/utils/_triton.py	2025-08-05 12:19:13.490482700 +1000
@@ -1,7 +1,6 @@
 # mypy: allow-untyped-defs
 import functools
 import hashlib
-from typing import Optional
 
 
 @functools.lru_cache(None)
@@ -51,8 +50,9 @@
         ):
             # old API
             try:
-                from triton.language import (  # noqa: F401
-                    _experimental_make_tensor_descriptor,
+                from triton.language.extra.cuda import (  # noqa: F401
+                    experimental_device_tensormap_create1d,
+                    experimental_device_tensormap_create2d,
                 )
 
                 return True
@@ -122,20 +122,3 @@
 
     # Hash is upper case so that it can't contain any Python keywords.
     return hashlib.sha256(key.encode("utf-8")).hexdigest().upper()
-
-
-@functools.lru_cache(None)
-def triton_set_allocator(device):
-    if has_triton_tma_device():
-        import torch
-
-        assert torch.cuda.current_device() == device
-
-        def alloc_fn(size: int, alignment: int, stream: Optional[int]):
-            return torch.empty(size, device=device, dtype=torch.int8)
-
-        import triton
-
-        triton.set_allocator(alloc_fn)
-
-    return None
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/collect_env.py torch/utils/collect_env.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/utils/collect_env.py	2025-06-10 07:37:20.000000000 +1000
+++ torch/utils/collect_env.py	2025-08-05 12:19:13.494302400 +1000
@@ -10,6 +10,7 @@
 import subprocess
 import sys
 import os
+from typing import cast as _cast
 from collections import namedtuple
 
 
@@ -37,6 +38,7 @@
     'nvidia_driver_version',
     'nvidia_gpu_models',
     'cudnn_version',
+    'is_xpu_available',
     'pip_version',  # 'pip' or 'pip3'
     'pip_packages',
     'conda_packages',
@@ -73,6 +75,30 @@
     "nvtx",
 ]
 
+ONEAPI_PATTERNS = [
+    "dpcpp-cpp-rt",
+    "intel-cmplr-lib-rt",
+    "intel-cmplr-lib-ur",
+    "intel-cmplr-lic-rt",
+    "intel-opencl-rt",
+    "intel-sycl-rt",
+    "mkl",
+    "onemkl-sycl-blas",
+    "onemkl-sycl-dft",
+    "onemkl-sycl-lapack",
+    "onemkl-sycl-rng",
+    "onemkl-sycl-sparse",
+    "intel-openmp",
+    "tbb",
+    "impi-rt",
+    "impi-devel",
+    "oneccl",
+    "oneccl-devel",
+    "intel-pti",
+    "umf",
+    "tcmlib",
+]
+
 CONDA_PATTERNS = [
     "cudatoolkit",
     "soumith",
@@ -131,7 +157,7 @@
 
 def get_conda_packages(run_lambda, patterns=None):
     if patterns is None:
-        patterns = CONDA_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS
+        patterns = CONDA_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS
     conda = os.environ.get('CONDA_EXE', 'conda')
     out = run_and_read_all(run_lambda, "{} list".format(conda))
     if out is None:
@@ -243,6 +269,149 @@
     return smi
 
 
+def _detect_linux_pkg_manager():
+    if get_platform() != "linux":
+        return "N/A"
+    for mgr_name in ["dpkg", "dnf", "yum", "zypper"]:
+        rc, _, _ = run(f"which {mgr_name}")
+        if rc == 0:
+            return mgr_name
+    return "N/A"
+
+
+def get_linux_pkg_version(run_lambda, pkg_name):
+    pkg_mgr = _detect_linux_pkg_manager()
+    if pkg_mgr == "N/A":
+        return "N/A"
+
+    grep_version = {
+        "dpkg": {
+            "field_index": 2,
+            "command": "dpkg -l | grep {}",
+        },
+        "dnf": {
+            "field_index": 1,
+            "command": "dnf list | grep {}",
+        },
+        "yum": {
+            "field_index": 1,
+            "command": "yum list | grep {}",
+        },
+        "zypper": {
+            "field_index": 2,
+            "command": "zypper info {} | grep Version",
+        },
+    }
+
+    field_index: int = int(_cast(int, grep_version[pkg_mgr]["field_index"]))
+    cmd: str = str(grep_version[pkg_mgr]["command"])
+    cmd = cmd.format(pkg_name)
+    ret = run_and_read_all(run_lambda, cmd)
+    if ret == "":
+        return "N/A"
+    lst = re.sub(" +", " ", ret).split(" ")
+    if len(lst) <= field_index:
+        return "N/A"
+    return lst[field_index]
+
+
+def get_intel_gpu_driver_version(run_lambda):
+    lst = []
+    platform = get_platform()
+    if platform == "linux":
+        pkgs = {  # type: ignore[var-annotated]
+            "dpkg": {
+                "intel-opencl-icd",
+                "libze1",
+            },
+            "dnf": {
+                "intel-opencl",
+                "level-zero",
+            },
+            "yum": {
+                "intel-opencl",
+                "level-zero",
+            },
+            "zypper": {
+                "intel-opencl",
+                "level-zero",
+            },
+        }.get(_detect_linux_pkg_manager(), {})
+        for pkg in pkgs:
+            lst.append(f"* {pkg}:\t{get_linux_pkg_version(run_lambda, pkg)}")
+    if platform in ["win32", "cygwin"]:
+        txt = run_and_read_all(
+            run_lambda,
+            'powershell.exe "gwmi -Class Win32_PnpSignedDriver | where{$_.DeviceClass -eq \\"DISPLAY\\"\
+            -and $_.Manufacturer -match \\"Intel\\"} | Select-Object -Property DeviceName,DriverVersion,DriverDate\
+            | ConvertTo-Json"',
+        )
+        try:
+            obj = json.loads(txt)
+            if type(obj) is list:
+                for o in obj:
+                    lst.append(
+                        f'* {o["DeviceName"]}: {o["DriverVersion"]} ({o["DriverDate"]})'
+                    )
+            else:
+                lst.append(f'* {obj["DriverVersion"]} ({obj["DriverDate"]})')
+        except ValueError as e:
+            lst.append(txt)
+            lst.append(str(e))
+    return "\n".join(lst)
+
+
+def get_intel_gpu_onboard(run_lambda):
+    lst: list[str] = []
+    platform = get_platform()
+    if platform == "linux":
+        txt = run_and_read_all(run_lambda, "xpu-smi discovery -j")
+        if txt:
+            try:
+                obj = json.loads(txt)
+                device_list = obj.get("device_list", [])
+                if isinstance(device_list, list) and device_list:
+                    lst.extend(f'* {device["device_name"]}' for device in device_list)
+                else:
+                    lst.append("N/A")
+            except (ValueError, TypeError) as e:
+                lst.append(txt)
+                lst.append(str(e))
+        else:
+            lst.append("N/A")
+    if platform in ["win32", "cygwin"]:
+        txt = run_and_read_all(
+            run_lambda,
+            'powershell.exe "gwmi -Class Win32_PnpSignedDriver | where{$_.DeviceClass -eq \\"DISPLAY\\"\
+            -and $_.Manufacturer -match \\"Intel\\"} | Select-Object -Property DeviceName | ConvertTo-Json"',
+        )
+        if txt:
+            try:
+                obj = json.loads(txt)
+                if isinstance(obj, list) and obj:
+                    lst.extend(f'* {device["DeviceName"]}' for device in obj)
+                else:
+                    lst.append(f'* {obj.get("DeviceName", "N/A")}')
+            except ValueError as e:
+                lst.append(txt)
+                lst.append(str(e))
+        else:
+            lst.append("N/A")
+    return "\n".join(lst)
+
+
+def get_intel_gpu_detected(run_lambda):
+    if not TORCH_AVAILABLE or not hasattr(torch, "xpu"):
+        return "N/A"
+
+    device_count = torch.xpu.device_count()
+    if device_count == 0:
+        return "N/A"
+
+    devices = [f"* [{i}] {torch.xpu.get_device_properties(i)}" for i in range(device_count)]
+    return "\n".join(devices)
+
+
 # example outputs of CPU infos
 #  * linux
 #    Architecture:            x86_64
@@ -396,7 +565,7 @@
     from platform import machine
     platform = get_platform()
 
-    if platform == 'win32' or platform == 'cygwin':
+    if platform in ["win32", "cygwin"]:
         return get_windows_version(run_lambda)
 
     if platform == 'darwin':
@@ -437,7 +606,7 @@
 def get_pip_packages(run_lambda, patterns=None):
     """Return `pip list` output. Note: will also find conda-installed pytorch and numpy packages."""
     if patterns is None:
-        patterns = PIP_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS
+        patterns = PIP_PATTERNS + COMMON_PATTERNS + NVIDIA_PATTERNS + ONEAPI_PATTERNS
 
     pip_version = 'pip3' if sys.version_info.major == 3 else 'pip'
 
@@ -504,6 +673,13 @@
         debug_mode_str = str(torch.version.debug)
         cuda_available_str = str(torch.cuda.is_available())
         cuda_version_str = torch.version.cuda
+        xpu_available_str = str(torch.xpu.is_available())
+        if torch.xpu.is_available():
+            xpu_available_str = f'{xpu_available_str}\n' + \
+                                f'XPU used to build PyTorch: {torch.version.xpu}\n' + \
+                                f'Intel GPU driver version:\n{get_intel_gpu_driver_version(run_lambda)}\n' + \
+                                f'Intel GPU models onboard:\n{get_intel_gpu_onboard(run_lambda)}\n' + \
+                                f'Intel GPU models detected:\n{get_intel_gpu_detected(run_lambda)}'
         if not hasattr(torch.version, 'hip') or torch.version.hip is None:  # cuda version
             hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'
         else:  # HIP version
@@ -517,7 +693,7 @@
             cuda_version_str = 'N/A'
             hip_compiled_version = torch.version.hip
     else:
-        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'
+        version_str = debug_mode_str = cuda_available_str = cuda_version_str = xpu_available_str = 'N/A'
         hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'
 
     sys_version = sys.version.replace("\n", " ")
@@ -536,6 +712,7 @@
         nvidia_gpu_models=get_gpu_info(run_lambda),
         nvidia_driver_version=get_nvidia_driver_version(run_lambda),
         cudnn_version=get_cudnn_version(run_lambda),
+        is_xpu_available=xpu_available_str,
         hip_compiled_version=hip_compiled_version,
         hip_runtime_version=hip_runtime_version,
         miopen_runtime_version=miopen_runtime_version,
@@ -572,6 +749,7 @@
 GPU models and configuration: {nvidia_gpu_models}
 Nvidia driver version: {nvidia_driver_version}
 cuDNN version: {cudnn_version}
+Is XPU available: {is_xpu_available}
 HIP runtime version: {hip_runtime_version}
 MIOpen runtime version: {miopen_runtime_version}
 Is XNNPACK available: {is_xnnpack_available}
diff -ru --ignore-trailing-space /tmp/tmp.WuYekIyeDt/unzipped/torch/version.py torch/version.py
--- /tmp/tmp.WuYekIyeDt/unzipped/torch/version.py	2025-06-10 07:59:04.000000000 +1000
+++ torch/version.py	2025-08-05 12:18:44.534820100 +1000
@@ -1,9 +1,9 @@
 from typing import Optional
 
 __all__ = ['__version__', 'debug', 'cuda', 'git_version', 'hip', 'xpu']
-__version__ = '2.8.0.dev20250610+cu118'
+__version__ = '2.8.0.dev20250608+cu118'
 debug = False
 cuda: Optional[str] = '11.8'
-git_version = '0f61f5f414165900730fd86cf58b6e1aae6f5b52'
+git_version = '843156205ec57cf78d5cf10bf982656b3db51dfd'
 hip: Optional[str] = None
 xpu: Optional[str] = None
